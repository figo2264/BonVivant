#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë…ë¦½ì ì¸ AI ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
strategy.pyì˜ ì˜ì¡´ì„± ì—†ì´ ì‹¤í–‰ ê°€ëŠ¥
"""

import pandas as pd
import numpy as np
import json
from datetime import datetime, timedelta
import ta
import warnings
warnings.filterwarnings('ignore')

# AI ëª¨ë¸ ì„í¬íŠ¸
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score
import lightgbm as lgb
import os
from sklearn.utils.class_weight import compute_class_weight
from sklearn.utils import resample

# ë°ì´í„° ì†ŒìŠ¤
import FinanceDataReader as fdr
from pykrx import stock as pystock

def get_past_data_total(n=1000):
    """ì „ì²´ ì‹œì¥ ê³¼ê±° ë°ì´í„° ì¡°íšŒ (pykrx ì‚¬ìš©)"""
    try:
        all_data = []
        end_date = datetime.now()
        current_date = end_date - timedelta(days=n)
        
        collected_days = 0
        max_collect_days = min(n, 365)  # ìµœëŒ€ 1ë…„ì¹˜ë§Œ ìˆ˜ì§‘
        
        while current_date <= end_date and collected_days < max_collect_days:
            if current_date.weekday() < 5:  # í‰ì¼ë§Œ
                try:
                    date_str = current_date.strftime('%Y%m%d')
                    
                    # ë°ì´í„° ìˆ˜ì§‘ ì‹œë„
                    try:
                        kospi = pystock.get_market_ohlcv(date_str, market='KOSPI')
                    except:
                        kospi = pd.DataFrame()
                    
                    try:
                        kosdaq = pystock.get_market_ohlcv(date_str, market='KOSDAQ')
                    except:
                        kosdaq = pd.DataFrame()
                    
                    if kospi.empty and kosdaq.empty:
                        current_date += timedelta(days=1)
                        continue
                        
                    daily_data = pd.concat([kospi, kosdaq])
                    
                    if not daily_data.empty and daily_data['ê±°ë˜ëŒ€ê¸ˆ'].sum() > 0:
                        # ì»¬ëŸ¼ëª… ë³€í™˜
                        daily_data = daily_data.rename(columns={
                            'ì‹œê°€': 'open', 'ê³ ê°€': 'high', 'ì €ê°€': 'low', 'ì¢…ê°€': 'close',
                            'ê±°ë˜ëŸ‰': 'volume', 'ê±°ë˜ëŒ€ê¸ˆ': 'trade_amount'
                        })
                        
                        daily_data['timestamp'] = current_date.strftime('%Y-%m-%d')
                        daily_data.index.name = 'ticker'
                        daily_data = daily_data.reset_index()
                        all_data.append(daily_data)
                        collected_days += 1
                        
                except Exception as e:
                    pass  # ë°ì´í„° ì—†ëŠ” ë‚ ì§œëŠ” ìŠ¤í‚µ
                    
            current_date += timedelta(days=1)
        
        if not all_data:
            print("âŒ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
            return pd.DataFrame()
        
        result = pd.concat(all_data, ignore_index=True)
        result['timestamp'] = pd.to_datetime(result['timestamp'])
        
        print(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(result)}ê°œ ë ˆì½”ë“œ, {result['ticker'].nunique()}ê°œ ì¢…ëª©")
        return result.sort_values(['timestamp', 'ticker']).reset_index(drop=True)
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

def create_technical_features(data):
    """ê°•í™”ëœ ê¸°ìˆ ì  ë¶„ì„ ì§€í‘œ ìƒì„±"""
    try:
        if len(data) < 30:
            # ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ ì§€í‘œë§Œ ìƒì„±
            data['return_1d'] = data['close'].pct_change(1)
            
            # ì¶”ê°€ ì§€í‘œë“¤
            data['candle_type'] = np.where(data['close'] > data['open'], 1, -1)
            data['candle_streak'] = data['candle_type'].rolling(3).sum()
            data['volume_price_corr'] = data['close'].rolling(20).corr(data['volume'])
            data['price_acceleration'] = data['return_1d'] - data['return_1d'].shift(1)
            data['round_number_proximity'] = (data['close'] % 1000) / 1000
            return data
        
        # ê¸°ë³¸ ìˆ˜ìµë¥  ê³„ì‚°
        for period in [1, 3, 5, 10, 20]:
            data[f'return_{period}d'] = data['close'].pct_change(period)

        # ì´ë™í‰ê·  ë° ë¹„ìœ¨
        for ma_period in [5, 10, 20, 60]:
            data[f'ma_{ma_period}'] = data['close'].rolling(ma_period).mean()
            data[f'price_ma_ratio_{ma_period}'] = data['close'] / data[f'ma_{ma_period}']

        # ê¸°ë³¸ ê¸°ìˆ ì  ì§€í‘œ
        data['rsi_14'] = ta.momentum.rsi(data['close'], window=14)
        data['rsi_30'] = ta.momentum.rsi(data['close'], window=30)
        data['volume_ratio_5d'] = data['volume'] / data['volume'].rolling(5).mean()
        data['volume_ratio_20d'] = data['volume'] / data['volume'].rolling(20).mean()
        data['volatility_10d'] = data['close'].pct_change().rolling(10).std()
        data['volatility_20d'] = data['close'].pct_change().rolling(20).std()

        # ë³¼ë¦°ì € ë°´ë“œ ê´€ë ¨ ì§€í‘œ
        bb_middle = data['close'].rolling(20).mean()
        bb_std = data['close'].rolling(20).std()
        data['bb_upper'] = bb_middle + (2 * bb_std)
        data['bb_lower'] = bb_middle - (2 * bb_std)
        data['bb_position'] = (data['close'] - bb_middle) / (2 * bb_std)
        data['bb_width'] = (data['bb_upper'] - data['bb_lower']) / bb_middle

        # MACD ì§€í‘œ
        try:
            macd_line = ta.trend.macd(data['close'])
            macd_signal = ta.trend.macd_signal(data['close'])
            data['macd'] = macd_line
            data['macd_signal'] = macd_signal
            data['macd_histogram'] = macd_line - macd_signal
        except:
            data['macd'] = 0
            data['macd_signal'] = 0
            data['macd_histogram'] = 0

        # ìŠ¤í† ìºìŠ¤í‹± ì§€í‘œ
        try:
            data['stoch_k'] = ta.momentum.stoch(data['high'], data['low'], data['close'])
            data['stoch_d'] = data['stoch_k'].rolling(3).mean()
        except:
            data['stoch_k'] = 50
            data['stoch_d'] = 50

        # ê°€ê²© ëª¨ë©˜í…€ ì§€í‘œ
        data['price_momentum_5'] = data['close'] / data['close'].shift(5) - 1
        data['price_momentum_10'] = data['close'] / data['close'].shift(10) - 1
        data['price_momentum_20'] = data['close'] / data['close'].shift(20) - 1

        # ê±°ë˜ëŸ‰ ê°€ì¤‘ í‰ê·  ê°€ê²© (VWAP)
        try:
            data['vwap'] = (data['close'] * data['volume']).rolling(20).sum() / data['volume'].rolling(20).sum()
            data['price_vwap_ratio'] = data['close'] / data['vwap']
        except:
            data['vwap'] = data['close']
            data['price_vwap_ratio'] = 1.0

        # ë³€ë™ì„± ê¸°ë°˜ ì§€í‘œ
        data['high_low_ratio'] = (data['high'] - data['low']) / data['close']
        data['close_open_ratio'] = data['close'] / data['open'] - 1

        # ì§€ì§€/ì €í•­ ë ˆë²¨ ê·¼ì ‘ë„
        data['recent_high_20'] = data['high'].rolling(20).max()
        data['recent_low_20'] = data['low'].rolling(20).min()
        data['high_proximity'] = (data['recent_high_20'] - data['close']) / data['recent_high_20']
        data['low_proximity'] = (data['close'] - data['recent_low_20']) / data['recent_low_20']

        # ì¶”ê°€ ì§€í‘œë“¤
        data['candle_type'] = np.where(data['close'] > data['open'], 1, -1)
        data['candle_streak'] = data['candle_type'].rolling(3).sum()
        data['volume_price_corr'] = data['close'].rolling(20).corr(data['volume'])
        data['price_acceleration'] = data['return_1d'] - data['return_1d'].shift(1)
        data['round_number_proximity'] = (data['close'] % 1000) / 1000
        
        return data
    except Exception as e:
        print(f"ê¸°ìˆ ì  ì§€í‘œ ìƒì„± ì˜¤ë¥˜: {e}")
        # ìµœì†Œí•œì˜ ì§€í‘œë¼ë„ ìƒì„±
        data['return_1d'] = data['close'].pct_change(1)
        data['candle_type'] = np.where(data['close'] > data['open'], 1, -1)
        data['candle_streak'] = data['candle_type'].rolling(3).sum()
        data['volume_price_corr'] = data['close'].rolling(20).corr(data['volume'])
        data['price_acceleration'] = data['return_1d'] - data['return_1d'].shift(1)
        data['round_number_proximity'] = (data['close'] % 1000) / 1000
        return data

def prepare_training_data(lookback_days=1000):
    """AI ëª¨ë¸ í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„"""
    print("ğŸ“š AI ëª¨ë¸ í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

    try:
        # ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘
        historical_data = get_past_data_total(n=lookback_days)

        if len(historical_data) < 300:
            print("âŒ í•™ìŠµ ë°ì´í„° ë¶€ì¡± (ìµœì†Œ 300ì¼ í•„ìš”)")
            return None, None

        all_features = []
        all_targets = []
        data_quality_stats = {
            'total_tickers': 0,
            'qualified_tickers': 0,
            'total_samples': 0,
            'skipped_insufficient_data': 0,
            'skipped_insufficient_features': 0,
            'skipped_insufficient_valid': 0
        }

        # ì¢…ëª©ë³„ë¡œ ë°ì´í„° ì²˜ë¦¬
        for ticker in historical_data['ticker'].unique():
            data_quality_stats['total_tickers'] += 1
            ticker_data = historical_data[historical_data['ticker'] == ticker].sort_values('timestamp')

            if len(ticker_data) < 30:
                data_quality_stats['skipped_insufficient_data'] += 1
                continue

            # ê°•í™”ëœ ê¸°ìˆ ì  ì§€í‘œ ìƒì„±
            ticker_data = create_technical_features(ticker_data.copy())

            # ë‹¤ì¤‘ ê¸°ê°„ ë¯¸ë˜ ìˆ˜ìµë¥  ê³„ì‚°
            for future_days in [3, 5, 7]:
                ticker_data[f'future_{future_days}d_return'] = ticker_data['close'].shift(-future_days) / ticker_data['close'] - 1

            # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì‚¬ìš©
            valid_data = ticker_data.dropna()

            if len(valid_data) < 20:
                data_quality_stats['skipped_insufficient_valid'] += 1
                continue

            data_quality_stats['qualified_tickers'] += 1

            # í•µì‹¬ í”¼ì²˜ë§Œ ì„ íƒ (25ê°œ)
            feature_columns = [
                'return_1d', 'return_3d', 'return_5d', 'return_10d', 'return_20d',
                'price_ma_ratio_5', 'price_ma_ratio_10', 'price_ma_ratio_20',
                'rsi_14', 'volume_ratio_5d', 'volume_ratio_20d',
                'volatility_10d', 'volatility_20d',
                'bb_position', 'bb_width', 'macd_histogram', 'stoch_k',
                'price_momentum_5', 'price_momentum_10', 'low_proximity',
                'candle_streak', 'volume_price_corr', 'price_acceleration', 'round_number_proximity'
            ]

            available_features = [col for col in feature_columns if col in valid_data.columns]

            if len(available_features) < 15:
                data_quality_stats['skipped_insufficient_features'] += 1
                continue

            features = valid_data[available_features].values

            # ê°œì„ ëœ íƒ€ê²Ÿ ìƒì„±: ì´ì§„ ë¶„ë¥˜
            future_5d_return = valid_data['future_5d_return']
            
            # ì´ì§„ ë¶„ë¥˜: 0(ì†ì‹¤/íš¡ë³´), 1(ìˆ˜ìµ)
            targets = np.where(future_5d_return >= 0.01, 1, 0)

            # ë¯¸ë˜ ë°ì´í„°ê°€ ì—†ëŠ” ë§ˆì§€ë§‰ 7ê°œ ì œì™¸
            if len(features) > 7:
                features = features[:-7]
                targets = targets[:-7]

                all_features.extend(features)
                all_targets.extend(targets)
                data_quality_stats['total_samples'] += len(features)

        if len(all_features) < 200:
            print("âŒ ì¶©ë¶„í•œ í•™ìŠµ ë°ì´í„° í™•ë³´ ì‹¤íŒ¨")
            print(f"ğŸ“Š ìˆ˜ì§‘ëœ ìƒ˜í”Œ: {len(all_features)}ê°œ (ìµœì†Œ 200ê°œ í•„ìš”)")
            return None, None

        print(f"âœ… í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
        print(f"   ğŸ“Š ì´ ìƒ˜í”Œ: {len(all_features)}ê°œ")
        print(f"   ğŸ“ˆ ë¶„ì„ ì¢…ëª©: {data_quality_stats['qualified_tickers']}/{data_quality_stats['total_tickers']}ê°œ")
        print(f"   ğŸ¯ íƒ€ê²Ÿ ë¶„í¬: {np.bincount(all_targets)}")
        print(f"   ğŸ“ í”¼ì²˜ ìˆ˜: {len(available_features) if 'available_features' in locals() else len(feature_columns)}ê°œ")
        print(f"   âŒ ìŠ¤í‚µ ì›ì¸:")
        print(f"      - ë°ì´í„° ë¶€ì¡±: {data_quality_stats['skipped_insufficient_data']}ê°œ")
        print(f"      - ìœ íš¨ ìƒ˜í”Œ ë¶€ì¡±: {data_quality_stats['skipped_insufficient_valid']}ê°œ")
        print(f"      - í”¼ì²˜ ë¶€ì¡±: {data_quality_stats['skipped_insufficient_features']}ê°œ")

        return np.array(all_features), np.array(all_targets)

    except Exception as e:
        print(f"âŒ ë°ì´í„° ì¤€ë¹„ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def train_ai_model():
    """AI ëª¨ë¸ í›ˆë ¨"""
    print("ğŸ¤– AI ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")

    # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
    X, y = prepare_training_data()

    if X is None or len(X) < 200:
        print("âŒ í•™ìŠµ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ëª¨ë¸ í›ˆë ¨ ë¶ˆê°€")
        return None

    try:
        # ë°ì´í„° ë¶„í•  (ì‹œê³„ì—´ ê³ ë ¤)
        n_samples = len(X)
        train_end = int(n_samples * 0.7)
        val_end = int(n_samples * 0.85)
        
        X_train = X[:train_end]
        y_train = y[:train_end]
        X_val = X[train_end:val_end]
        y_val = y[train_end:val_end]
        X_test = X[val_end:]
        y_test = y[val_end:]

        # í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°
        X_train_class0 = X_train[y_train == 0]
        X_train_class1 = X_train[y_train == 1]
        
        if len(X_train_class0) > len(X_train_class1):
            X_train_class0_resampled = resample(X_train_class0, n_samples=int(len(X_train_class1) * 1.5), random_state=42)
            X_train_class1_resampled = X_train_class1
        else:
            X_train_class0_resampled = X_train_class0
            X_train_class1_resampled = resample(X_train_class1, n_samples=int(len(X_train_class0) * 1.5), random_state=42)
        
        X_train = np.vstack([X_train_class0_resampled, X_train_class1_resampled])
        y_train = np.hstack([
            np.zeros(len(X_train_class0_resampled)),
            np.ones(len(X_train_class1_resampled))
        ])
        
        shuffle_idx = np.random.permutation(len(X_train))
        X_train = X_train[shuffle_idx]
        y_train = y_train[shuffle_idx].astype(int)

        print(f"ğŸ“Š ë°ì´í„° ë¶„í• : í›ˆë ¨({len(X_train)}) / ê²€ì¦({len(X_val)}) / í…ŒìŠ¤íŠ¸({len(X_test)})")

        # LightGBM íŒŒë¼ë¯¸í„°
        lgb_params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'min_data_in_leaf': 30,
            'lambda_l1': 0.1,
            'lambda_l2': 0.1,
            'min_gain_to_split': 0.05,
            'max_depth': 6,
            'verbose': -1,
            'random_state': 42,
            'force_col_wise': True,
            'is_unbalance': True,
            'boost_from_average': True,
        }

        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
        class_weights = compute_class_weight(
            'balanced',
            classes=np.unique(y_train),
            y=y_train
        )
        
        print(f"ğŸ“Š í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {dict(zip(np.unique(y_train), class_weights))}")
        sample_weights = np.array([class_weights[label] for label in y_train])

        # ë°ì´í„°ì…‹ ìƒì„±
        train_data = lgb.Dataset(X_train, label=y_train, weight=sample_weights)
        valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

        # ëª¨ë¸ í›ˆë ¨
        model = lgb.train(
            lgb_params,
            train_data,
            valid_sets=[valid_data],
            num_boost_round=500,
            callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(50)]
        )

        # ì„±ëŠ¥ í‰ê°€
        y_pred_val_proba = model.predict(X_val)
        y_pred_test_proba = model.predict(X_test)
        
        y_pred_val = (y_pred_val_proba > 0.5).astype(int)
        y_pred_test = (y_pred_test_proba > 0.5).astype(int)
        
        val_accuracy = accuracy_score(y_val, y_pred_val)
        test_accuracy = accuracy_score(y_test, y_pred_test)

        print(f"âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
        print(f"ğŸ“Š ê²€ì¦ ì •í™•ë„: {val_accuracy:.3f}")
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.3f}")
        
        print(f"ğŸ“Š ê²€ì¦ ë°ì´í„° í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬:")
        print(f"   ì‹¤ì œ: {np.bincount(y_val)}")
        print(f"   ì˜ˆì¸¡: {np.bincount(y_pred_val)}")

        # ì¶”ê°€ ì„±ëŠ¥ ì§€í‘œ
        try:
            auc_score = roc_auc_score(y_test, y_pred_test_proba)
            print(f"ğŸ“Š AUC ì ìˆ˜: {auc_score:.3f}")
        except:
            auc_score = 0.5

        f1 = f1_score(y_test, y_pred_test)
        precision = precision_score(y_test, y_pred_test)
        recall = recall_score(y_test, y_pred_test)
        
        # ëª¨ë¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        model_quality_score = 0
        model_quality_score += min(auc_score * 30, 30)
        model_quality_score += f1 * 30
        model_quality_score += precision * 20
        model_quality_score += recall * 20
        
        print(f"ğŸ† ëª¨ë¸ í’ˆì§ˆ ì ìˆ˜: {model_quality_score:.1f}/100")
        print(f"   ğŸ“Š AUC: {auc_score:.3f}")
        print(f"   ğŸ“Š ì •í™•ë„: {test_accuracy:.3f}")
        print(f"   ğŸ“Š ì •ë°€ë„: {precision:.3f}")
        print(f"   ğŸ“Š ì¬í˜„ìœ¨: {recall:.3f}")
        print(f"   ğŸ“Š F1 ì ìˆ˜: {f1:.3f}")
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(y_test, y_pred_test)
        print(f"   ğŸ“Š í˜¼ë™ í–‰ë ¬:")
        print(f"      ì˜ˆì¸¡ 0    ì˜ˆì¸¡ 1")
        print(f"   ì‹¤ì œ 0: {cm[0,0]:6d} {cm[0,1]:6d}")
        print(f"   ì‹¤ì œ 1: {cm[1,0]:6d} {cm[1,1]:6d}")

        # ëª¨ë¸ ì €ì¥
        model.save_model('ai_price_prediction_model.txt')
        print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: ai_price_prediction_model.txt")

        # ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì €ì¥
        model_metadata = {
            'train_date': datetime.now().isoformat(),
            'model_type': 'LightGBM_Binary_Independent',
            'train_samples': len(X_train),
            'test_accuracy': float(test_accuracy),
            'val_accuracy': float(val_accuracy),
            'model_quality_score': float(model_quality_score),
            'feature_count': X_train.shape[1],
            'class_count': 2,
            'auc_score': float(auc_score),
            'f1_score': float(f1),
            'precision': float(precision),
            'recall': float(recall),
            'class_distribution': {
                'train': np.bincount(y_train).tolist(),
                'test': np.bincount(y_test).tolist()
            },
            'prediction_distribution': {
                'val': np.bincount(y_pred_val).tolist(),
                'test': np.bincount(y_pred_test).tolist()
            }
        }
        
        with open('ai_model_metadata.json', 'w') as f:
            json.dump(model_metadata, f, indent=2)

        return model

    except Exception as e:
        print(f"âŒ ëª¨ë¸ í›ˆë ¨ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë…ë¦½ì ì¸ AI ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
    
    # ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì‚­ì œ (ê°•ì œ ì¬í›ˆë ¨)
    model_files = ['ai_price_prediction_model.txt', 'ai_model_metadata.json']
    for file in model_files:
        if os.path.exists(file):
            os.remove(file)
            print(f"ğŸ—‘ï¸ ê¸°ì¡´ {file} ì‚­ì œ")
    
    # AI ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰
    model = train_ai_model()
    
    if model is not None:
        print("âœ… AI ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
        print("ğŸ“Š ìƒˆë¡œìš´ ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ëª¨ë¸ ë©”íƒ€ë°ì´í„° í™•ì¸
        try:
            with open('ai_model_metadata.json', 'r') as f:
                metadata = json.load(f)
                print(f"ğŸ“ˆ ëª¨ë¸ í’ˆì§ˆ ì ìˆ˜: {metadata.get('model_quality_score', 0):.1f}/100")
                print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {metadata.get('test_accuracy', 0):.3f}")
        except:
            pass
        
        return True
    else:
        print("âŒ AI ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨!")
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\nğŸ‰ ëª¨ë¸ í›ˆë ¨ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ ì´ì œ strategy.pyë¥¼ ì‹¤í–‰í•˜ë©´ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        print("\nğŸ’¥ ëª¨ë¸ í›ˆë ¨ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
