#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
Backtest Engine Performance Benchmark
"""

import sys
import os
import time
import json
import psutil
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, Any, List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ê¸°ì¡´ ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ê³¼ ëª¨ë“ˆí™”ëœ ì—”ì§„ ì„í¬íŠ¸
try:
    from backtest_engine import BacktestEngine as OriginalBacktestEngine
    ORIGINAL_ENGINE_AVAILABLE = True
except ImportError:
    ORIGINAL_ENGINE_AVAILABLE = False
    print("âš ï¸ ê¸°ì¡´ ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

from hanlyang_stock.backtest import BacktestEngine as ModularBacktestEngine
from hanlyang_stock.config.backtest_settings import get_backtest_config


class BacktestBenchmark:
    """ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.benchmark_results = {}
        self.test_scenarios = [
            {
                'name': 'ë‹¨ê¸° í…ŒìŠ¤íŠ¸ (5ì¼)',
                'days': 5,
                'ai_enabled': False,
                'description': 'ë¹ ë¥¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸'
            },
            {
                'name': 'ì¤‘ê¸° í…ŒìŠ¤íŠ¸ (30ì¼)',
                'days': 30,
                'ai_enabled': False,
                'description': 'ê¸°ë³¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸'
            },
            {
                'name': 'AI ë‹¨ê¸° í…ŒìŠ¤íŠ¸ (5ì¼)',
                'days': 5,
                'ai_enabled': True,
                'description': 'AI ê¸°ëŠ¥ í¬í•¨ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸'
            },
            {
                'name': 'AI ì¤‘ê¸° í…ŒìŠ¤íŠ¸ (30ì¼)',
                'days': 30,
                'ai_enabled': True,
                'description': 'AI ê¸°ëŠ¥ í¬í•¨ ê¸°ë³¸ í…ŒìŠ¤íŠ¸'
            }
        ]
    
    def measure_performance(self, func, *args, **kwargs) -> Dict[str, Any]:
        """í•¨ìˆ˜ ì‹¤í–‰ ì„±ëŠ¥ ì¸¡ì •"""
        # ì‹œì‘ ì „ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        process = psutil.Process(os.getpid())
        start_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        
        try:
            result = func(*args, **kwargs)
            success = True
            error = None
        except Exception as e:
            result = None
            success = False
            error = str(e)
        
        end_time = time.time()
        
        # ì¢…ë£Œ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        end_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        return {
            'execution_time': end_time - start_time,
            'start_memory_mb': start_memory,
            'end_memory_mb': end_memory,
            'memory_used_mb': end_memory - start_memory,
            'success': success,
            'error': error,
            'result': result
        }
    
    def run_modular_backtest(self, start_date: str, end_date: str, ai_enabled: bool = False) -> Dict[str, Any]:
        """ëª¨ë“ˆí™”ëœ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"ğŸ”§ ëª¨ë“ˆí™”ëœ ì—”ì§„ ì‹¤í–‰: {start_date} ~ {end_date} (AI: {ai_enabled})")
        
        config = get_backtest_config('balanced')
        engine = ModularBacktestEngine(
            initial_capital=config.initial_capital,
            transaction_cost=config.transaction_cost
        )
        
        results = engine.run_backtest(start_date, end_date, ai_enabled=ai_enabled)
        return results
    
    def run_original_backtest(self, start_date: str, end_date: str, ai_enabled: bool = False) -> Dict[str, Any]:
        """ê¸°ì¡´ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        if not ORIGINAL_ENGINE_AVAILABLE:
            raise ImportError("ê¸°ì¡´ ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"ğŸ“Š ê¸°ì¡´ ì—”ì§„ ì‹¤í–‰: {start_date} ~ {end_date} (AI: {ai_enabled})")
        
        engine = OriginalBacktestEngine(
            initial_capital=10_000_000,
            transaction_cost=0.003
        )
        
        results = engine.run_backtest(start_date, end_date, ai_enabled=ai_enabled)
        return results
    
    def compare_results(self, original_results: Dict[str, Any], 
                       modular_results: Dict[str, Any]) -> Dict[str, Any]:
        """ê²°ê³¼ ë¹„êµ"""
        comparison = {}
        
        # ê¸°ë³¸ ì„±ê³¼ ì§€í‘œ ë¹„êµ
        metrics_to_compare = [
            'total_return', 'total_trades', 'win_rate', 
            'max_drawdown', 'avg_profit_per_trade'
        ]
        
        for metric in metrics_to_compare:
            original_value = original_results.get(metric, 0)
            modular_value = modular_results.get(metric, 0)
            
            # ì°¨ì´ ê³„ì‚°
            if isinstance(original_value, (int, float)) and isinstance(modular_value, (int, float)):
                if original_value != 0:
                    difference_pct = ((modular_value - original_value) / original_value) * 100
                else:
                    difference_pct = 0 if modular_value == 0 else float('inf')
                
                comparison[metric] = {
                    'original': original_value,
                    'modular': modular_value,
                    'difference': modular_value - original_value,
                    'difference_pct': difference_pct
                }
        
        return comparison
    
    def run_benchmark_scenario(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """ê°œë³„ ë²¤ì¹˜ë§ˆí¬ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰"""
        print(f"\nğŸ ë²¤ì¹˜ë§ˆí¬ ì‹œë‚˜ë¦¬ì˜¤: {scenario['name']}")
        print(f"   ì„¤ëª…: {scenario['description']}")
        print("=" * 50)
        
        # í…ŒìŠ¤íŠ¸ ë‚ ì§œ ì„¤ì •
        end_date = datetime.now()
        start_date = end_date - timedelta(days=scenario['days'])
        start_str = start_date.strftime('%Y-%m-%d')
        end_str = end_date.strftime('%Y-%m-%d')
        
        scenario_results = {
            'scenario': scenario,
            'test_period': {'start': start_str, 'end': end_str}
        }
        
        # ëª¨ë“ˆí™”ëœ ì—”ì§„ í…ŒìŠ¤íŠ¸
        print("ğŸ”§ ëª¨ë“ˆí™”ëœ ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ í…ŒìŠ¤íŠ¸...")
        modular_perf = self.measure_performance(
            self.run_modular_backtest,
            start_str, end_str, scenario['ai_enabled']
        )
        scenario_results['modular_engine'] = modular_perf
        
        if modular_perf['success']:
            print(f"   âœ… ì™„ë£Œ: {modular_perf['execution_time']:.2f}ì´ˆ")
            print(f"   ğŸ’¾ ë©”ëª¨ë¦¬: {modular_perf['memory_used_mb']:.1f}MB ì‚¬ìš©")
            
            modular_results = modular_perf['result']
            if modular_results:
                print(f"   ğŸ“ˆ ìˆ˜ìµë¥ : {modular_results.get('total_return', 0)*100:+.2f}%")
                print(f"   ğŸ”„ ê±°ë˜: {modular_results.get('total_trades', 0)}íšŒ")
        else:
            print(f"   âŒ ì‹¤íŒ¨: {modular_perf['error']}")
        
        # ê¸°ì¡´ ì—”ì§„ í…ŒìŠ¤íŠ¸ (ê°€ëŠ¥í•œ ê²½ìš°)
        if ORIGINAL_ENGINE_AVAILABLE:
            print("\nğŸ“Š ê¸°ì¡´ ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ í…ŒìŠ¤íŠ¸...")
            original_perf = self.measure_performance(
                self.run_original_backtest,
                start_str, end_str, scenario['ai_enabled']
            )
            scenario_results['original_engine'] = original_perf
            
            if original_perf['success']:
                print(f"   âœ… ì™„ë£Œ: {original_perf['execution_time']:.2f}ì´ˆ")
                print(f"   ğŸ’¾ ë©”ëª¨ë¦¬: {original_perf['memory_used_mb']:.1f}MB ì‚¬ìš©")
                
                original_results = original_perf['result']
                if original_results:
                    print(f"   ğŸ“ˆ ìˆ˜ìµë¥ : {original_results.get('total_return', 0)*100:+.2f}%")
                    print(f"   ğŸ”„ ê±°ë˜: {original_results.get('total_trades', 0)}íšŒ")
                
                # ì„±ëŠ¥ ë¹„êµ
                if modular_perf['success'] and modular_perf['result'] and original_results:
                    comparison = self.compare_results(original_results, modular_perf['result'])
                    scenario_results['comparison'] = comparison
                    
                    print("\nğŸ“Š ê²°ê³¼ ë¹„êµ:")
                    speed_improvement = ((original_perf['execution_time'] - modular_perf['execution_time']) 
                                       / original_perf['execution_time']) * 100
                    memory_difference = modular_perf['memory_used_mb'] - original_perf['memory_used_mb']
                    
                    print(f"   â±ï¸ ì‹¤í–‰ ì‹œê°„: {speed_improvement:+.1f}% {'ê°œì„ ' if speed_improvement > 0 else 'ì•…í™”'}")
                    print(f"   ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©: {memory_difference:+.1f}MB {'ì¦ê°€' if memory_difference > 0 else 'ê°ì†Œ'}")
                    
                    # ì£¼ìš” ì§€í‘œ ë¹„êµ
                    for metric, comp in comparison.items():
                        if abs(comp['difference_pct']) > 0.1:  # 0.1% ì´ìƒ ì°¨ì´
                            print(f"   ğŸ“Š {metric}: {comp['difference_pct']:+.2f}% ì°¨ì´")
            else:
                print(f"   âŒ ì‹¤íŒ¨: {original_perf['error']}")
        else:
            scenario_results['original_engine'] = None
            print("\nğŸ“Š ê¸°ì¡´ ì—”ì§„ í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ (ì—”ì§„ ì—†ìŒ)")
        
        return scenario_results
    
    def run_full_benchmark(self):
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸ ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        print("=" * 60)
        print(f"í…ŒìŠ¤íŠ¸ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ì‹œìŠ¤í…œ ì •ë³´: Python {sys.version_info.major}.{sys.version_info.minor}")
        print(f"ë©”ëª¨ë¦¬: {psutil.virtual_memory().total / 1024 / 1024 / 1024:.1f}GB")
        print(f"CPU: {psutil.cpu_count()}ì½”ì–´")
        
        benchmark_start_time = time.time()
        
        # ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        for i, scenario in enumerate(self.test_scenarios, 1):
            print(f"\nğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤ {i}/{len(self.test_scenarios)}")
            scenario_results = self.run_benchmark_scenario(scenario)
            self.benchmark_results[scenario['name']] = scenario_results
        
        benchmark_end_time = time.time()
        total_benchmark_time = benchmark_end_time - benchmark_start_time
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        self.print_benchmark_summary(total_benchmark_time)
        
        # ê²°ê³¼ ì €ì¥
        self.save_benchmark_results()
    
    def print_benchmark_summary(self, total_time: float):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ğŸ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        print(f"ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ìš”ì•½
        print("\nğŸ“Š ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥:")
        for scenario_name, results in self.benchmark_results.items():
            print(f"\nğŸ”¸ {scenario_name}")
            
            modular = results.get('modular_engine', {})
            original = results.get('original_engine', {})
            
            if modular.get('success'):
                print(f"   ğŸ”§ ëª¨ë“ˆí™”ëœ ì—”ì§„: {modular['execution_time']:.2f}ì´ˆ")
                
                if original and original.get('success'):
                    print(f"   ğŸ“Š ê¸°ì¡´ ì—”ì§„: {original['execution_time']:.2f}ì´ˆ")
                    
                    speed_diff = modular['execution_time'] - original['execution_time']
                    speed_pct = (speed_diff / original['execution_time']) * 100
                    
                    if speed_pct < -5:
                        print(f"   ğŸš€ {abs(speed_pct):.1f}% ë” ë¹ ë¦„")
                    elif speed_pct > 5:
                        print(f"   ğŸŒ {speed_pct:.1f}% ë” ëŠë¦¼")
                    else:
                        print(f"   âš–ï¸ ë¹„ìŠ·í•œ ì„±ëŠ¥ ({speed_pct:+.1f}%)")
                else:
                    print("   ğŸ“Š ê¸°ì¡´ ì—”ì§„: í…ŒìŠ¤íŠ¸ ë¶ˆê°€")
            else:
                print(f"   âŒ ëª¨ë“ˆí™”ëœ ì—”ì§„: ì‹¤íŒ¨ ({modular.get('error', 'Unknown')})")
        
        # ì „ì²´ ì„±ëŠ¥ í‰ê°€
        print("\nğŸ¯ ì¢…í•© í‰ê°€:")
        
        successful_tests = sum(1 for results in self.benchmark_results.values() 
                             if results.get('modular_engine', {}).get('success', False))
        total_tests = len(self.benchmark_results)
        
        print(f"   ì„±ê³µë¥ : {successful_tests}/{total_tests} ({(successful_tests/total_tests)*100:.1f}%)")
        
        if ORIGINAL_ENGINE_AVAILABLE:
            # í‰ê·  ì„±ëŠ¥ ë¹„êµ
            speed_comparisons = []
            for results in self.benchmark_results.values():
                modular = results.get('modular_engine', {})
                original = results.get('original_engine', {})
                
                if (modular.get('success') and original and original.get('success')):
                    speed_ratio = modular['execution_time'] / original['execution_time']
                    speed_comparisons.append(speed_ratio)
            
            if speed_comparisons:
                avg_speed_ratio = sum(speed_comparisons) / len(speed_comparisons)
                if avg_speed_ratio < 0.95:
                    print(f"   ğŸš€ í‰ê·  {(1-avg_speed_ratio)*100:.1f}% ì„±ëŠ¥ í–¥ìƒ")
                elif avg_speed_ratio > 1.05:
                    print(f"   ğŸŒ í‰ê·  {(avg_speed_ratio-1)*100:.1f}% ì„±ëŠ¥ ì €í•˜")
                else:
                    print(f"   âš–ï¸ ê¸°ì¡´ ì—”ì§„ê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥")
        
        print("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        if successful_tests == total_tests:
            print("   âœ… ëª¨ë“ˆí™”ëœ ì—”ì§„ì´ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
            print("   âœ… í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        else:
            print("   âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¬¸ì œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            print("   âš ï¸ ì¶”ê°€ í…ŒìŠ¤íŠ¸ í›„ í”„ë¡œë•ì…˜ ì ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        print("=" * 60)
    
    def save_benchmark_results(self, filename: str = None):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥"""
        if filename is None:
            filename = f"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # ê²°ê³¼ ì§ë ¬í™” (datetime, pandas ê°ì²´ ì²˜ë¦¬)
        def serialize_object(obj):
            if isinstance(obj, datetime):
                return obj.isoformat()
            elif isinstance(obj, pd.Timestamp):
                return obj.isoformat()
            elif isinstance(obj, dict):
                return {key: serialize_object(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [serialize_object(item) for item in obj]
            elif hasattr(obj, '__dict__'):
                return serialize_object(obj.__dict__)
            else:
                return obj
        
        benchmark_summary = {
            'benchmark_date': datetime.now().isoformat(),
            'system_info': {
                'python_version': f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
                'total_memory_gb': psutil.virtual_memory().total / 1024 / 1024 / 1024,
                'cpu_count': psutil.cpu_count(),
                'original_engine_available': ORIGINAL_ENGINE_AVAILABLE
            },
            'test_scenarios': self.test_scenarios,
            'results': serialize_object(self.benchmark_results)
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(benchmark_summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥: {filename}")
        return filename


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    
    benchmark = BacktestBenchmark()
    
    try:
        benchmark.run_full_benchmark()
        
    except KeyboardInterrupt:
        print("\në²¤ì¹˜ë§ˆí¬ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
