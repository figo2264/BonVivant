"""
ë‰´ìŠ¤ ê°ì • ë¶„ì„ ê¸°ë°˜ ì£¼ê°€ ì˜ˆì¸¡ ëª¨ë“ˆ
News sentiment analysis based stock price prediction module
"""

import requests
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional
import pandas as pd
import json
from bs4 import BeautifulSoup
import os
from anthropic import Anthropic
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from webdriver_manager.chrome import ChromeDriverManager
import time
import random
from pykrx import stock

# .env íŒŒì¼ ìˆ˜ë™ ë¡œë“œ
def load_env():
    env_path = Path(__file__).parent.parent.parent / '.env'
    if env_path.exists():
        with open(env_path, 'r') as f:
            for line in f:
                if line.strip() and not line.startswith('#'):
                    key, value = line.strip().split('=', 1)
                    os.environ[key] = value


class NewsAnalyzer:
    """ë‰´ìŠ¤ ê°ì • ë¶„ì„ í´ë˜ìŠ¤ - Claude API ì‚¬ìš©"""
    
    def __init__(self, api_key: Optional[str] = None, debug: bool = False):
        """
        Initialize NewsAnalyzer with Claude API
        
        Args:
            api_key: Anthropic API key
            debug: ë””ë²„ê·¸ ëª¨ë“œ
        """
        # .env íŒŒì¼ ë¡œë“œ
        load_env()
        
        self.api_key = api_key or os.getenv('ANTHROPIC_API_KEY')
        self.debug = debug
        
        if self.api_key:
            self.client = Anthropic(api_key=self.api_key)
        else:
            self.client = None
            print("âš ï¸ Claude API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‰´ìŠ¤ ë¶„ì„ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
        
        # ì¢…ëª©ë³„ íšŒì‚¬ëª… ìºì‹œ
        self.company_name_cache = {}
    
    def fetch_ticker_news_old(self, ticker: str, company_name: str, date: str) -> List[Dict[str, str]]:
        """
        íŠ¹ì • ì¢…ëª©ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ (ë§¤ì¼ê²½ì œ)
        
        Args:
            ticker: ì¢…ëª© ì½”ë“œ (ì˜ˆ: '005930')
            company_name: íšŒì‚¬ëª… (ì˜ˆ: 'ì‚¼ì„±ì „ì')
            date: ë‚ ì§œ (YYYY-MM-DD)
            
        Returns:
            List[Dict]: ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ [{title, date, url}, ...]
        """
        news_list = []
        
        # íšŒì‚¬ëª…ì´ ì¢…ëª©ì½”ë“œì™€ ê°™ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ì‹¤ì œ íšŒì‚¬ëª… ì¡°íšŒ
        if company_name == ticker or not company_name:
            # ìºì‹œ í™•ì¸
            if ticker in self.company_name_cache:
                company_name = self.company_name_cache[ticker]
                if self.debug:
                    print(f"  ğŸ¢ ìºì‹œì—ì„œ íšŒì‚¬ëª… ì¡°íšŒ: {ticker} â†’ {company_name}")
            else:
                try:
                    actual_company_name = stock.get_market_ticker_name(ticker)
                    if actual_company_name:
                        company_name = actual_company_name
                        self.company_name_cache[ticker] = company_name
                        print(f"  ğŸ¢ ì¢…ëª© ì½”ë“œ {ticker} â†’ íšŒì‚¬ëª…: {company_name}")
                except Exception as e:
                    print(f"  âš ï¸ íšŒì‚¬ëª… ì¡°íšŒ ì‹¤íŒ¨ ({ticker}): {e}")
                    # íšŒì‚¬ëª…ì„ ëª¨ë¥´ë©´ ì¢…ëª© ì½”ë“œë¡œë¼ë„ ê²€ìƒ‰ ì‹œë„
        
        # Selenium ë“œë¼ì´ë²„ ì„¤ì •
        serv = Service(ChromeDriverManager().install())
        chrome_options = webdriver.ChromeOptions()
        
        # ë””ë²„ê·¸ ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°ë§Œ headless
        if not self.debug:
            chrome_options.add_argument('--headless')
        
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        # User-Agent ì„¤ì • (ë´‡ìœ¼ë¡œ ì¸ì‹ë˜ëŠ” ê²ƒ ë°©ì§€)
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36')
        
        driver = webdriver.Chrome(service=serv, options=chrome_options)
        
        try:
            # ë§¤ì¼ê²½ì œ AI ê²€ìƒ‰ URL (ë” ê°„ë‹¨í•œ êµ¬ì¡°)
            url = f'https://www.mk.co.kr/aisearch?word={company_name}'
            
            print(f"  ğŸ“¡ ë§¤ì¼ê²½ì œ AI ê²€ìƒ‰ URL: {url}")
            driver.get(url)
            
            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            time.sleep(3)
            
            # ê²€ìƒ‰ ê²°ê³¼ ëŒ€ê¸° ë° íŒŒì‹±
            try:
                # í˜ì´ì§€ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸° - h3 íƒœê·¸ë‚˜ result í´ë˜ìŠ¤ê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€
                WebDriverWait(driver, 10).until(
                    lambda driver: driver.find_elements(By.CSS_SELECTOR, 'h3.news_ttl, [class*="result"], div[class*="news"]')
                )
                
                # ì¶”ê°€ ëŒ€ê¸° (ë™ì  ì»¨í…ì¸  ë¡œë”©)
                time.sleep(2)
                
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                
                # ë§¤ì¼ê²½ì œ AI ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ì°¾ê¸°
                news_items = []
                
                # ë°©ë²• 1: h3.news_ttl ì„ íƒìë¡œ ë‰´ìŠ¤ ì œëª© ì§ì ‘ ì°¾ê¸°
                news_titles = soup.select('h3.news_ttl')
                print(f"  ğŸ“Š h3.news_ttlë¡œ {len(news_titles)}ê°œ ë‰´ìŠ¤ ì œëª© ë°œê²¬")
                
                # ë°©ë²• 2: ë‰´ìŠ¤ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
                if not news_titles:
                    news_containers = soup.select('div[class*="news"]')
                    print(f"  ğŸ“Š div[class*='news']ë¡œ {len(news_containers)}ê°œ ì»¨í…Œì´ë„ˆ ë°œê²¬")
                    
                    # ê° ì»¨í…Œì´ë„ˆì—ì„œ ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ
                    for container in news_containers:
                        title_elem = container.find('h3') or container.find('h2') or container.find('h4')
                        if title_elem:
                            news_titles.append(title_elem)
                
                # ë°©ë²• 3: ê¸°ì‚¬ ë§í¬ê°€ ìˆëŠ” ëª¨ë“  í•­ëª© ì°¾ê¸°
                if not news_titles:
                    article_links = soup.select('a[href*="/news/"], a[href*="/article/"]')
                    print(f"  ğŸ“Š ê¸°ì‚¬ ë§í¬ë¡œ {len(article_links)}ê°œ ë°œê²¬")
                    
                    # ë§í¬ì—ì„œ ì œëª©ì´ ìˆëŠ” ê²ƒë§Œ ì¶”ì¶œ
                    for link in article_links:
                        if link.text.strip() and len(link.text.strip()) > 10:
                            news_titles.append(link)
                
                print(f"  ğŸ“Š ì´ {len(news_titles)}ê°œ ë‰´ìŠ¤ ì œëª© ìˆ˜ì§‘")
                
                # ë‰´ìŠ¤ ì •ë³´ ì¶”ì¶œ
                for idx, title_elem in enumerate(news_titles[:20]):  # ìµœê·¼ 20ê°œë§Œ ì¶”ì¶œ
                    try:
                        # ì œëª© ì¶”ì¶œ
                        title = title_elem.text.strip()
                        
                        # ì œëª©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ AI ë‹µë³€ ë“± ë¶ˆí•„ìš”í•œ ë‚´ìš© í•„í„°ë§
                        if not title or len(title) < 10:
                            continue
                        
                        # AI ê´€ë ¨ ì»¨í…ì¸  í•„í„°ë§
                        exclude_keywords = ['AI ë‹µë³€', 'ì°¸ê³ ìë£Œ', 'ê´€ë ¨ ì§ˆë¬¸', 'Powered by', 'perplexity']
                        if any(keyword in title for keyword in exclude_keywords):
                            continue
                        
                        # ì‹¤ì œ ë‰´ìŠ¤ ì œëª© íŒ¨í„´ í™•ì¸ (ë³´í†µ íšŒì‚¬ëª…ì´ë‚˜ ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨)
                        if company_name not in title and ticker not in title:
                            # ì œëª©ì— íšŒì‚¬ëª…ì´ë‚˜ ì¢…ëª©ì½”ë“œê°€ ì—†ìœ¼ë©´ ê´€ë ¨ì„± ë‚®ìŒ
                            # í•˜ì§€ë§Œ ì™„ì „íˆ ì œì™¸í•˜ì§€ëŠ” ì•ŠìŒ (ê´€ë ¨ ì—…ê³„ ë‰´ìŠ¤ì¼ ìˆ˜ ìˆìŒ)
                            pass
                        
                        # URL ì¶”ì¶œ - ì œëª© ìš”ì†Œë‚˜ ë¶€ëª¨ ìš”ì†Œì—ì„œ ë§í¬ ì°¾ê¸°
                        news_url = ''
                        
                        # ì œëª© ìš”ì†Œ ìì²´ê°€ ë§í¬ì¸ ê²½ìš°
                        if title_elem.name == 'a' and title_elem.get('href'):
                            news_url = title_elem['href']
                        else:
                            # ë¶€ëª¨ë‚˜ í˜•ì œ ìš”ì†Œì—ì„œ ë§í¬ ì°¾ê¸°
                            parent = title_elem.parent
                            while parent and parent.name != 'body':
                                link = parent.find('a', href=True)
                                if link and link.get('href'):
                                    news_url = link['href']
                                    break
                                parent = parent.parent
                        
                        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                        if news_url and not news_url.startswith('http'):
                            if news_url.startswith('//'):
                                news_url = 'https:' + news_url
                            else:
                                news_url = 'https://www.mk.co.kr' + news_url
                        
                        # ë‚ ì§œ ì¶”ì¶œ - í˜„ì¬ ë‚ ì§œë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
                        news_date = date
                        
                        # ë¶€ëª¨ ìš”ì†Œì—ì„œ ë‚ ì§œ ì •ë³´ ì°¾ê¸°
                        parent = title_elem.parent
                        while parent and parent.name != 'body':
                            date_text = parent.find(string=lambda x: x and any(year in str(x) for year in ['2023', '2024', '2025']))
                            if date_text:
                                date_str = str(date_text).strip()
                                # ë‚ ì§œ í˜•ì‹ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
                                if len(date_str) < 20 and ('.' in date_str or '-' in date_str or 'ë…„' in date_str):
                                    news_date = date_str
                                    break
                            parent = parent.parent
                        
                        news_list.append({
                            'title': title,
                            'date': news_date,
                            'url': news_url
                        })
                        
                        print(f"  âœ… ë‰´ìŠ¤ {idx+1}: {title[:50]}...")
                        
                    except Exception as e:
                        print(f"  âš ï¸ ë‰´ìŠ¤ í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                
                # ë” ë§ì€ ë‰´ìŠ¤ ë¡œë“œ ì‹œë„ (ë¬´í•œ ìŠ¤í¬ë¡¤ì´ë‚˜ ë”ë³´ê¸° ë²„íŠ¼)
                if len(news_list) < 10:
                    print(f"  ğŸ“Œ ë‰´ìŠ¤ê°€ {len(news_list)}ê°œë¿ì´ë¯€ë¡œ ì¶”ê°€ ë¡œë“œ ì‹œë„...")
                    
                    # 1. ìŠ¤í¬ë¡¤ì„ ë‚´ë ¤ì„œ ë” ë§ì€ ë‰´ìŠ¤ ë¡œë“œ ì‹œë„
                    for _ in range(3):  # 3ë²ˆ ìŠ¤í¬ë¡¤
                        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                        time.sleep(1.5)
                    
                    # 2. ë”ë³´ê¸° ë²„íŠ¼ ì°¾ê¸° - ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
                    try:
                        more_button_selectors = [
                            'button:contains("ë”ë³´ê¸°")',
                            'a:contains("ë”ë³´ê¸°")',
                            'button[class*="more"]',
                            'a[class*="more"]',
                            'button.btn_more',
                            'a.btn_more',
                            'div[class*="more"] button',
                            'div[class*="more"] a'
                        ]
                        
                        for selector in more_button_selectors:
                            try:
                                # JavaScriptë¡œ ìš”ì†Œ ì°¾ê¸° (contains ì„ íƒì ì§€ì›)
                                if ':contains(' in selector:
                                    text = selector.split('"')[1]
                                    element_type = selector.split(':')[0]
                                    buttons = driver.find_elements(By.TAG_NAME, element_type)
                                    for button in buttons:
                                        if text in button.text and button.is_displayed():
                                            button.click()
                                            print(f"  âœ… ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì„±ê³µ: {button.text}")
                                            time.sleep(2)
                                            break
                                else:
                                    # CSS ì„ íƒìë¡œ ì°¾ê¸°
                                    buttons = driver.find_elements(By.CSS_SELECTOR, selector)
                                    for button in buttons:
                                        if button.is_displayed():
                                            button.click()
                                            print(f"  âœ… ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
                                            time.sleep(2)
                                            break
                            except:
                                continue
                    except Exception as e:
                        print(f"  âš ï¸ ë”ë³´ê¸° ë²„íŠ¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    
                    # 3. í˜ì´ì§€ ì¬íŒŒì‹±í•˜ì—¬ ì¶”ê°€ ë‰´ìŠ¤ ìˆ˜ì§‘
                    soup = BeautifulSoup(driver.page_source, 'html.parser')
                    
                    # ì¶”ê°€ ë‰´ìŠ¤ ì œëª© ì°¾ê¸°
                    additional_titles = soup.select('h3.news_ttl')
                    for title_elem in additional_titles[len(news_list):]:  # ì´ë¯¸ ìˆ˜ì§‘í•œ ê²ƒ ì´í›„ë¶€í„°
                        try:
                            title = title_elem.text.strip()
                            if not title or len(title) < 10:
                                continue
                            
                            # URLê³¼ ë‚ ì§œ ì¶”ì¶œ (ìœ„ì™€ ë™ì¼í•œ ë¡œì§)
                            news_url = ''
                            if title_elem.name == 'a' and title_elem.get('href'):
                                news_url = title_elem['href']
                            else:
                                parent = title_elem.parent
                                while parent and parent.name != 'body':
                                    link = parent.find('a', href=True)
                                    if link and link.get('href'):
                                        news_url = link['href']
                                        break
                                    parent = parent.parent
                            
                            if news_url and not news_url.startswith('http'):
                                if news_url.startswith('//'):
                                    news_url = 'https:' + news_url
                                else:
                                    news_url = 'https://www.mk.co.kr' + news_url
                            
                            news_list.append({
                                'title': title,
                                'date': date,
                                'url': news_url
                            })
                        except Exception as e:
                            continue
                
            except Exception as e:
                print(f"  âš ï¸ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                # ë””ë²„ê¹…ì„ ìœ„í•´ í˜ì´ì§€ ì†ŒìŠ¤ ì¼ë¶€ ì¶œë ¥
                print(f"  ğŸ“„ í˜ì´ì§€ ì œëª©: {driver.title}")
                print(f"  ğŸ“„ URL: {driver.current_url}")
                
                # í˜ì´ì§€ êµ¬ì¡° ë””ë²„ê¹…
                try:
                    soup = BeautifulSoup(driver.page_source, 'html.parser')
                    
                    # AI ê´€ë ¨ ìš”ì†Œ í™•ì¸
                    ai_elements = soup.select('[class*="ai"], [id*="ai"]')
                    print(f"  ğŸ“„ AI ê´€ë ¨ ìš”ì†Œ: {len(ai_elements)}ê°œ")
                    
                    # ë‰´ìŠ¤ ê´€ë ¨ ìš”ì†Œ í™•ì¸
                    news_elements = soup.select('h3.news_ttl, div[class*="news"], [class*="result"]')
                    print(f"  ğŸ“„ ë‰´ìŠ¤ ê´€ë ¨ ìš”ì†Œ: {len(news_elements)}ê°œ")
                    
                    # ì²« ë²ˆì§¸ h3 ìš”ì†Œë“¤ ì¶œë ¥
                    h3_elements = soup.find_all('h3')[:5]
                    print(f"  ğŸ“„ ì²« 5ê°œ h3 ìš”ì†Œ:")
                    for i, h3 in enumerate(h3_elements):
                        print(f"    {i+1}. {h3.text.strip()[:50]}...")
                except:
                    pass
                
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜ ({ticker}): {e}")
            
        finally:
            driver.quit()
        
        print(f"  âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(news_list)}ê°œ ë‰´ìŠ¤")
        return news_list
    
    def fetch_ticker_news(self, ticker: str, company_name: str, date: str) -> List[Dict[str, str]]:
        """
        ë„¤ì´ë²„ ì¦ê¶Œì—ì„œ ì¢…ëª© ë‰´ìŠ¤ë¥¼ Seleniumìœ¼ë¡œ í¬ë¡¤ë§
        
        Args:
            ticker: ì¢…ëª© ì½”ë“œ (ì˜ˆ: '005930')
            company_name: íšŒì‚¬ëª… (ì˜ˆ: 'ì‚¼ì„±ì „ì')
            date: ê¸°ì¤€ ë‚ ì§œ (YYYY-MM-DD)
            
        Returns:
            List[Dict]: ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ [{title, date, url, source}, ...]
        """
        print(f"ğŸ“¡ ë„¤ì´ë²„ ì¦ê¶Œì—ì„œ {ticker} ({company_name}) ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        
        # ê²½ì œ ì „ë¬¸ ë§¤ì²´ ë¦¬ìŠ¤íŠ¸
        QUALITY_SOURCES = [
            'í•œêµ­ê²½ì œ', 'í•œê²½', 'ì—°í•©ì¸í¬ë§¥ìŠ¤', 'ì¸í¬ë§¥ìŠ¤',
            'ë§¤ì¼ê²½ì œ', 'ë§¤ê²½', 'ì„œìš¸ê²½ì œ', 'ì´ë°ì¼ë¦¬',
            'ë¨¸ë‹ˆíˆ¬ë°ì´', 'íŒŒì´ë‚¸ì…œë‰´ìŠ¤', 'ì•„ì‹œì•„ê²½ì œ',
            'í—¤ëŸ´ë“œê²½ì œ', 'ì¡°ì„ ë¹„ì¦ˆ', 'ë‰´ìŠ¤1', 'ë‰´ì‹œìŠ¤'
        ]
        
        news_list = []
        driver = None
        
        try:
            # Selenium ë“œë¼ì´ë²„ ì„¤ì •
            chrome_options = webdriver.ChromeOptions()
            if not self.debug:
                chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
            
            # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™” (ì†ë„ í–¥ìƒ)
            prefs = {"profile.managed_default_content_settings.images": 2}
            chrome_options.add_experimental_option("prefs", prefs)
            
            serv = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=serv, options=chrome_options)
            
            # ë„¤ì´ë²„ ì¦ê¶Œ ë‰´ìŠ¤ í˜ì´ì§€
            url = f"https://finance.naver.com/item/news.naver?code={ticker}"
            print(f"  ğŸ“„ í˜ì´ì§€ ì ‘ì†: {url}")
            
            driver.get(url)
            
            # iframe ëŒ€ê¸° ë° ì „í™˜
            try:
                # iframeì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.ID, "news_frame"))
                )
                
                # iframeìœ¼ë¡œ ì „í™˜
                news_iframe = driver.find_element(By.ID, "news_frame")
                driver.switch_to.frame(news_iframe)
                print("  âœ… ë‰´ìŠ¤ í”„ë ˆì„ ì§„ì… ì„±ê³µ")
                
                # ë‰´ìŠ¤ í…Œì´ë¸” ëŒ€ê¸°
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "table.type5"))
                )
                
            except Exception as e:
                print(f"  âš ï¸ iframe ì „í™˜ ì‹¤íŒ¨: {e}")
                # iframe ì—†ì´ ì‹œë„
                pass
            
            # í˜ì´ì§€ ì†ŒìŠ¤ íŒŒì‹±
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            # ë‰´ìŠ¤ í…Œì´ë¸” ì°¾ê¸°
            news_table = soup.find('table', class_='type5')
            if not news_table:
                print("  âš ï¸ ë‰´ìŠ¤ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ë‰´ìŠ¤ í–‰ ì°¾ê¸°
            rows = news_table.find_all('tr')
            print(f"  ğŸ“Š {len(rows)}ê°œ í–‰ ë°œê²¬")
            
            news_count = 0
            
            for row in rows:
                # í—¤ë” í–‰ ìŠ¤í‚µ
                if row.find('th'):
                    continue
                
                # ì œëª© ì…€ ì°¾ê¸°
                title_cell = row.find('td', class_='title')
                if not title_cell:
                    continue
                
                # ì œëª©ê³¼ ë§í¬
                link_elem = title_cell.find('a')
                if not link_elem:
                    continue
                
                title = link_elem.text.strip()
                news_url = link_elem.get('href', '')
                
                # ì ˆëŒ€ URLë¡œ ë³€í™˜
                if news_url and not news_url.startswith('http'):
                    if news_url.startswith('//'):
                        news_url = 'https:' + news_url
                    else:
                        news_url = 'https://finance.naver.com' + news_url
                
                # ì •ë³´ ì œê³µì (ì–¸ë¡ ì‚¬)
                info_cell = row.find('td', class_='info')
                source = info_cell.text.strip() if info_cell else 'ì•Œ ìˆ˜ ì—†ìŒ'
                
                # ë‚ ì§œ
                date_cell = row.find('td', class_='date')
                news_date = date_cell.text.strip() if date_cell else date
                
                # ê²½ì œ ì „ë¬¸ì§€ í•„í„°ë§
                is_quality = any(s in source for s in QUALITY_SOURCES)
                
                if not is_quality and not self.debug:
                    continue
                
                news_item = {
                    'title': title,
                    'date': news_date,
                    'url': news_url,
                    'source': source
                }
                
                news_list.append(news_item)
                news_count += 1
                
                if self.debug:
                    print(f"  âœ… [{source}] {title[:50]}...")
                
                # ìµœëŒ€ 20ê°œë§Œ ìˆ˜ì§‘
                if news_count >= 20:
                    break
            
            print(f"  ğŸ“° {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
            
            # ë‚ ì§œìˆœ ì •ë ¬
            news_list.sort(key=lambda x: x['date'], reverse=True)
            
            # ìµœëŒ€ 10ê°œë§Œ ë°˜í™˜
            final_news = news_list[:10]
            
            # ì†ŒìŠ¤ë³„ í†µê³„
            if final_news:
                source_stats = {}
                for news in final_news:
                    src = news['source']
                    source_stats[src] = source_stats.get(src, 0) + 1
                
                print(f"  ğŸ“Š ì†ŒìŠ¤ë³„ ë¶„í¬:")
                for src, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True):
                    print(f"     - {src}: {count}ê°œ")
            
            return final_news
            
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜ ({ticker}): {e}")
            if self.debug:
                import traceback
                traceback.print_exc()
            
            # Selenium ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback
            print(f"  âš ï¸ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„...")
            return self.fetch_ticker_news_old(ticker, company_name, date)
            
        finally:
            if driver:
                driver.quit()
    
    def fetch_ticker_news_requests(self, ticker: str, company_name: str, date: str) -> List[Dict[str, str]]:
        """
        ë„¤ì´ë²„ ì¦ê¶Œì—ì„œ ì¢…ëª© ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ê³  ê²½ì œ ì „ë¬¸ ë§¤ì²´ë§Œ í•„í„°ë§
        
        Args:
            ticker: ì¢…ëª© ì½”ë“œ (ì˜ˆ: '005930')
            company_name: íšŒì‚¬ëª… (ì˜ˆ: 'ì‚¼ì„±ì „ì')
            date: ê¸°ì¤€ ë‚ ì§œ (YYYY-MM-DD)
            
        Returns:
            List[Dict]: ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ [{title, date, url, source}, ...]
        """
        print(f"ğŸ“¡ ë„¤ì´ë²„ ì¦ê¶Œì—ì„œ {ticker} ({company_name}) ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        
        # ê²½ì œ ì „ë¬¸ ë§¤ì²´ ë¦¬ìŠ¤íŠ¸ (í•œê²½, ì—°í•©ì¸í¬ë§¥ìŠ¤ ì¤‘ì‹¬)
        QUALITY_SOURCES = [
            'í•œêµ­ê²½ì œ', 'í•œê²½', 'í•œêµ­ê²½ì œì‹ ë¬¸', 'hankyung',
            'ì—°í•©ì¸í¬ë§¥ìŠ¤', 'ì¸í¬ë§¥ìŠ¤', 'einfomax',
            'ë§¤ì¼ê²½ì œ', 'ë§¤ê²½', 'mk',
            'ì„œìš¸ê²½ì œ', 'ì„œê²½', 'sedaily',
            'ë¨¸ë‹ˆíˆ¬ë°ì´', 'moneytoday',
            'ì´ë°ì¼ë¦¬', 'edaily',
            'íŒŒì´ë‚¸ì…œë‰´ìŠ¤', 'fnnews',
            'ì•„ì‹œì•„ê²½ì œ', 'asiae'
        ]
        
        news_list = []
        
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì • (ê¸°ë³¸ 7ì¼)
        days_back = 7
        end_date = datetime.strptime(date, '%Y-%m-%d')
        start_date = end_date - timedelta(days=days_back)
        
        # ìµœì í™”ëœ í—¤ë”
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        
        # ë„¤ì´ë²„ ì¦ê¶Œ ë‰´ìŠ¤ URL
        base_url = "https://finance.naver.com/item/news_news.naver"
        
        try:
            # í˜ì´ì§€ë³„ë¡œ í¬ë¡¤ë§ (ìµœëŒ€ 5í˜ì´ì§€)
            for page in range(1, 6):
                # ìš”ì²­ ê°„ê²© ì¡°ì ˆ (ë¡œë´‡ ì°¨ë‹¨ íšŒí”¼)
                if page > 1:
                    time.sleep(random.uniform(1.0, 2.5))
                
                # URL íŒŒë¼ë¯¸í„°
                params = {
                    'code': ticker,
                    'page': page,
                    'sm': 'entity_id.basic',
                    'clusterId': ''
                }
                
                # ìš”ì²­ ì „ì†¡
                response = requests.get(base_url, params=params, headers=headers)
                
                if response.status_code != 200:
                    print(f"  âš ï¸ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨ (ìƒíƒœ ì½”ë“œ: {response.status_code})")
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ë‰´ìŠ¤ í…Œì´ë¸” ì°¾ê¸°
                news_table = soup.find('table', class_='type5')
                if not news_table:
                    print(f"  âš ï¸ í˜ì´ì§€ {page}ì— ë‰´ìŠ¤ í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                # ë‰´ìŠ¤ í•­ëª© ì¶”ì¶œ
                rows = news_table.find_all('tr')
                page_news_count = 0
                
                for row in rows:
                    # ì œëª©ê³¼ ë§í¬ê°€ ìˆëŠ” í–‰ë§Œ ì²˜ë¦¬
                    title_cell = row.find('td', class_='title')
                    if not title_cell:
                        continue
                    
                    link_elem = title_cell.find('a')
                    if not link_elem:
                        continue
                    
                    # ë‰´ìŠ¤ ì •ë³´ ì¶”ì¶œ
                    title = link_elem.text.strip()
                    news_url = 'https://finance.naver.com' + link_elem.get('href', '')
                    
                    # ì •ë³´ ì œê³µì ì¶”ì¶œ
                    info_cell = row.find('td', class_='info')
                    source = info_cell.text.strip() if info_cell else 'ì•Œ ìˆ˜ ì—†ìŒ'
                    
                    # ë‚ ì§œ ì¶”ì¶œ
                    date_cell = row.find('td', class_='date')
                    news_date_str = date_cell.text.strip() if date_cell else ''
                    
                    # ë‚ ì§œ íŒŒì‹± ë° í•„í„°ë§
                    try:
                        # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
                        if 'ì‹œê°„' in news_date_str or 'ë¶„' in news_date_str:
                            # ì˜¤ëŠ˜ ë‚ ì§œ
                            news_date = end_date
                        elif '.' in news_date_str:
                            # 2024.01.15 í˜•ì‹
                            if len(news_date_str.split('.')[0]) == 2:
                                # 24.01.15 í˜•ì‹
                                news_date_str = '20' + news_date_str
                            news_date = datetime.strptime(news_date_str.split()[0], '%Y.%m.%d')
                        else:
                            # ë‹¤ë¥¸ í˜•ì‹ì€ ì˜¤ëŠ˜ë¡œ ê°€ì •
                            news_date = end_date
                        
                        # ë‚ ì§œ ë²”ìœ„ ì²´í¬
                        if news_date < start_date:
                            if self.debug:
                                print(f"  â­ï¸ ë‚ ì§œ ë²”ìœ„ ë²—ì–´ë‚¨: {news_date_str}")
                            continue
                            
                    except Exception as e:
                        if self.debug:
                            print(f"  âš ï¸ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {news_date_str} - {e}")
                        news_date = end_date
                    
                    # ê²½ì œ ì „ë¬¸ ë§¤ì²´ í•„í„°ë§
                    is_quality_source = any(
                        src.lower() in source.lower() 
                        for src in QUALITY_SOURCES
                    )
                    
                    if not is_quality_source:
                        if self.debug:
                            print(f"  â­ï¸ í•„í„°ë§ë¨: {source} - {title[:30]}...")
                        continue
                    
                    # ë‰´ìŠ¤ ì¶”ê°€
                    news_item = {
                        'title': title,
                        'date': news_date.strftime('%Y-%m-%d'),
                        'url': news_url,
                        'source': source
                    }
                    
                    news_list.append(news_item)
                    page_news_count += 1
                    
                    if self.debug:
                        print(f"  âœ… [{source}] {title[:50]}...")
                
                print(f"  ğŸ“„ í˜ì´ì§€ {page}: {page_news_count}ê°œ ìˆ˜ì§‘")
                
                # ë‰´ìŠ¤ê°€ ì—†ê±°ë‚˜ ë‚ ì§œ ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ê²½ìš° ì¤‘ë‹¨
                if page_news_count == 0:
                    break
            
            # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
            unique_news = []
            seen_titles = set()
            
            for news in news_list:
                title_key = news['title'][:50]  # ì• 50ìë¡œ ì¤‘ë³µ ì²´í¬
                if title_key not in seen_titles:
                    seen_titles.add(title_key)
                    unique_news.append(news)
            
            # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
            unique_news.sort(key=lambda x: x['date'], reverse=True)
            
            # ìµœëŒ€ 10ê°œë§Œ ë°˜í™˜ (Claude API ë¹„ìš© ì ˆê°)
            final_news = unique_news[:10]
            
            print(f"  âœ… ìµœì¢… ìˆ˜ì§‘: {len(final_news)}ê°œ (ì „ì²´ {len(unique_news)}ê°œ ì¤‘)")
            
            # ì†ŒìŠ¤ë³„ í†µê³„
            if final_news:
                source_stats = {}
                for news in final_news:
                    src = news['source']
                    source_stats[src] = source_stats.get(src, 0) + 1
                
                print(f"  ğŸ“Š ì†ŒìŠ¤ë³„ ë¶„í¬:")
                for src, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True):
                    print(f"     - {src}: {count}ê°œ")
            
            return final_news
            
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜ ({ticker}): {e}")
            if self.debug:
                import traceback
                traceback.print_exc()
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback
            print(f"  âš ï¸ ë„¤ì´ë²„ ì¦ê¶Œ í¬ë¡¤ë§ ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„...")
            return self.fetch_ticker_news_old(ticker, company_name, date)
    
    def analyze_news_sentiment(self, news_list: List[Dict], ticker: str, company_name: str, 
                               days: List[int] = [1, 5, 10, 20]) -> Dict[str, any]:
        """
        Claude APIë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ê°ì • ë¶„ì„ ë° ì£¼ê°€ ì˜ˆì¸¡
        
        Args:
            news_list: ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
            ticker: ì¢…ëª© ì½”ë“œ
            company_name: íšŒì‚¬ëª…
            days: ì˜ˆì¸¡í•  ì¼ìˆ˜ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict: ì˜ˆì¸¡ ê²°ê³¼
        """
        # ë””ë²„ê¹…: ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ìƒíƒœ í™•ì¸
        if self.debug:
            print(f"  [DEBUG] ë‰´ìŠ¤ ê°œìˆ˜: {len(news_list) if news_list else 0}")
            print(f"  [DEBUG] Claude í´ë¼ì´ì–¸íŠ¸: {'ìˆìŒ' if self.client else 'ì—†ìŒ'}")
        
        if not self.client or not news_list:
            if self.debug:
                print(f"  [DEBUG] ê¸°ë³¸ê°’ ë°˜í™˜ - í´ë¼ì´ì–¸íŠ¸: {bool(self.client)}, ë‰´ìŠ¤: {bool(news_list)}")
            return self._get_default_predictions(days)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„± - ì†ŒìŠ¤ ì •ë³´ í¬í•¨
        news_info = []
        for i, news in enumerate(news_list[:10]):
            source = news.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
            title = news['title']
            news_info.append(f"{i+1}. [{source}] {title}")
        
        news_titles = "\n".join(news_info)
        
        prompt = f"""ë‹¤ìŒì€ {company_name}({ticker}) ê´€ë ¨ ìµœê·¼ ë‰´ìŠ¤ì…ë‹ˆë‹¤:

{news_titles}

ìœ„ ë‰´ìŠ¤ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì£¼ê°€ ì˜í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë¶„ì„ ì‹œ ê³ ë ¤ì‚¬í•­:
1. ë‰´ìŠ¤ ì¶œì²˜ì˜ ì‹ ë¢°ë„: í•œêµ­ê²½ì œ, ì—°í•©ì¸í¬ë§¥ìŠ¤ ë“± ê²½ì œ ì „ë¬¸ì§€ëŠ” ë†’ì€ ê°€ì¤‘ì¹˜
2. ë‰´ìŠ¤ ë‚´ìš©ì˜ êµ¬ì²´ì„±: ì‹¤ì , ê³„ì•½, íˆ¬ì ë“± êµ¬ì²´ì  ìˆ˜ì¹˜ê°€ ìˆëŠ” ë‰´ìŠ¤ëŠ” ì¤‘ìš”
3. ì‹œì¥ ë°˜ì‘ ì˜ˆì¸¡: ê¸°ê´€íˆ¬ììì™€ ê°œì¸íˆ¬ììì˜ ë°˜ì‘ ì°¨ì´ ê³ ë ¤

ë‹¨ê³„ë³„ ë¶„ì„:
1. ë‰´ìŠ¤ ë¶„ë¥˜: ê° ë‰´ìŠ¤ë¥¼ ê¸ì •ì /ë¶€ì •ì /ì¤‘ë¦½ì ìœ¼ë¡œ ë¶„ë¥˜í•˜ê³  ì¤‘ìš”ë„ í‰ê°€
2. ì‹œê°„ë³„ ì˜í–¥: ì¦‰ê°ì  ë°˜ì‘(1ì¼), ë‹¨ê¸° ëª¨ë©˜í…€(5ì¼), ì¤‘ê¸° íŠ¸ë Œë“œ(10-20ì¼) êµ¬ë¶„
3. ì‹ ë¢°ë„ í‰ê°€: ë‰´ìŠ¤ì˜ ì¶œì²˜ì™€ êµ¬ì²´ì„±ì„ ê³ ë ¤í•œ ì˜ˆì¸¡ ì‹ ë¢°ë„

ì£¼ê°€ ìƒìŠ¹ ê°€ëŠ¥ì„± í‰ê°€ ê¸°ì¤€:
- 50% ë¯¸ë§Œ: í•˜ë½ ê°€ëŠ¥ì„±ì´ ë” ë†’ìŒ
- 50-60%: ë³´í•© ë˜ëŠ” ì†Œí­ ìƒìŠ¹
- 60-70%: ìƒìŠ¹ ê°€ëŠ¥ì„± ìˆìŒ
- 70% ì´ìƒ: ê°•í•œ ìƒìŠ¹ ê¸°ëŒ€

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”:
- {days[0]}ì¼ í›„ ìƒìŠ¹ í™•ë¥ : XX%
- {days[1]}ì¼ í›„ ìƒìŠ¹ í™•ë¥ : XX%
- {days[2]}ì¼ í›„ ìƒìŠ¹ í™•ë¥ : XX%
- {days[3]}ì¼ í›„ ìƒìŠ¹ í™•ë¥ : XX%
- ì¢…í•© ê°ì •: ê¸ì •/ì¤‘ë¦½/ë¶€ì •
- ì£¼ìš” ì´ìœ : (ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” ë‰´ìŠ¤ ìš”ì¸ì„ 50ì ì´ë‚´ë¡œ)"""
        
        try:
            # Claude API í˜¸ì¶œ
            if self.debug:
                print(f"  [DEBUG] Claude API í˜¸ì¶œ ì‹œì‘...")
            
            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=500,
                temperature=0.2,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            # ë””ë²„ê¹…: ì‘ë‹µ ë‚´ìš© í™•ì¸
            response_text = response.content[0].text
            if self.debug:
                print(f"  [DEBUG] Claude ì‘ë‹µ ê¸¸ì´: {len(response_text)}ì")
                print(f"  [DEBUG] Claude ì‘ë‹µ ì²« 100ì: {response_text[:100]}...")
            
            # ì‘ë‹µ íŒŒì‹±
            return self._parse_claude_response(response_text, days)
        
        except Exception as e:
            print(f"âŒ Claude API ì˜¤ë¥˜: {e}")
            return self._get_default_predictions(days)
    
    def _parse_claude_response(self, response_text: str, days: List[int]) -> Dict[str, any]:
        """Claude ì‘ë‹µ íŒŒì‹±"""
        predictions = {}
        
        # ë””ë²„ê¹…: íŒŒì‹± ì‹œì‘
        if self.debug:
            print(f"  [DEBUG] ì‘ë‹µ íŒŒì‹± ì‹œì‘...")
        
        try:
            lines = response_text.strip().split('\n')
            if self.debug:
                print(f"  [DEBUG] ì‘ë‹µ ë¼ì¸ ìˆ˜: {len(lines)}")
            
            # ê° ì¼ìˆ˜ë³„ í™•ë¥  ì¶”ì¶œ
            for i, day in enumerate(days):
                for line in lines:
                    if f"{day}ì¼ í›„" in line:
                        prob = float(line.split(':')[1].strip().replace('%', ''))
                        predictions[f'prob_{day}'] = prob / 100.0
                        if self.debug:
                            print(f"  [DEBUG] {day}ì¼ í™•ë¥  íŒŒì‹±: {prob}%")
                        break
            
            # ì¢…í•© ê°ì • ì¶”ì¶œ
            for line in lines:
                if 'ì¢…í•© ê°ì •:' in line:
                    sentiment = line.split(':')[1].strip()
                    predictions['sentiment'] = sentiment
                    if self.debug:
                        print(f"  [DEBUG] ê°ì • íŒŒì‹±: {sentiment}")
                elif 'ì£¼ìš” ì´ìœ :' in line:
                    reason = line.split(':')[1].strip()
                    predictions['reason'] = reason
            
            # í‰ê·  í™•ë¥  ê³„ì‚°
            prob_values = [v for k, v in predictions.items() if k.startswith('prob_')]
            predictions['avg_confidence'] = sum(prob_values) / len(prob_values) if prob_values else 0.5
            if self.debug:
                print(f"  [DEBUG] íŒŒì‹±ëœ í™•ë¥  ê°œìˆ˜: {len(prob_values)}")
                print(f"  [DEBUG] í‰ê·  ì‹ ë¢°ë„: {predictions['avg_confidence'] * 100:.1f}%")
            
        except Exception as e:
            print(f"âš ï¸ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return self._get_default_predictions(days)
        
        return predictions
    
    def _get_default_predictions(self, days: List[int]) -> Dict[str, any]:
        """ê¸°ë³¸ ì˜ˆì¸¡ê°’ ë°˜í™˜"""
        predictions = {
            'avg_confidence': 0.5,
            'sentiment': 'ì¤‘ë¦½',
            'reason': 'ë‰´ìŠ¤ ë¶„ì„ ë¶ˆê°€'
        }
        
        for day in days:
            predictions[f'prob_{day}'] = 0.5
        
        return predictions
    
    def find_optimal_parameters(self, historical_data: pd.DataFrame, 
                                days_list: List[int] = [1, 5, 10, 20],
                                threshold_list: List[float] = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8]) -> Tuple[int, float]:
        """
        ê³¼ê±° ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ ë³´ìœ  ê¸°ê°„(n)ê³¼ ë§¤ìˆ˜ ê¸°ì¤€ í™•ë¥ (threshold) ì°¾ê¸°
        
        Args:
            historical_data: ê³¼ê±° ë‰´ìŠ¤ ë° ì£¼ê°€ ë°ì´í„°
            days_list: í…ŒìŠ¤íŠ¸í•  ë³´ìœ  ê¸°ê°„ ë¦¬ìŠ¤íŠ¸
            threshold_list: í…ŒìŠ¤íŠ¸í•  ë§¤ìˆ˜ ê¸°ì¤€ í™•ë¥  ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Tuple[int, float]: (ìµœì  ë³´ìœ  ê¸°ê°„, ìµœì  ë§¤ìˆ˜ ê¸°ì¤€ í™•ë¥ )
        """
        best_return = -float('inf')
        best_n = days_list[0]
        best_threshold = threshold_list[0]
        
        for n in days_list:
            for threshold in threshold_list:
                # ë°±í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
                total_return = self._backtest_strategy(historical_data, n, threshold)
                
                if total_return > best_return:
                    best_return = total_return
                    best_n = n
                    best_threshold = threshold
        
        print(f"ğŸ¯ ìµœì  íŒŒë¼ë¯¸í„°: ë³´ìœ ê¸°ê°„={best_n}ì¼, ë§¤ìˆ˜ê¸°ì¤€={best_threshold:.2f}, ìˆ˜ìµë¥ ={best_return:.2%}")
        
        return best_n, best_threshold
    
    def _backtest_strategy(self, data: pd.DataFrame, n: int, threshold: float) -> float:
        """ê°„ë‹¨í•œ ë°±í…ŒìŠ¤íŠ¸ êµ¬í˜„"""
        initial_capital = 10_000_000
        capital = initial_capital
        position = 0
        
        # ì‹¤ì œ ë°±í…ŒìŠ¤íŠ¸ ë¡œì§ì€ ë” ë³µì¡í•˜ê²Œ êµ¬í˜„ í•„ìš”
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë§Œ ì œê³µ
        
        return (capital - initial_capital) / initial_capital
    
    def analyze_ticker_news(self, ticker: str, company_name: str, date: str) -> Dict[str, any]:
        """
        íŠ¹ì • ì¢…ëª©ì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„
        
        Args:
            ticker: ì¢…ëª© ì½”ë“œ
            company_name: íšŒì‚¬ëª…
            date: ë‚ ì§œ (YYYY-MM-DD)
            
        Returns:
            Dict: ë¶„ì„ ê²°ê³¼
        """
        # ë‰´ìŠ¤ ìˆ˜ì§‘
        news_list = self.fetch_ticker_news(ticker, company_name, date)
        
        # ê°ì • ë¶„ì„ ë° ì˜ˆì¸¡
        return self.analyze_news_sentiment(news_list, ticker, company_name)


# í¸ì˜ í•¨ìˆ˜ë“¤
_analyzer_instance = None

def get_news_analyzer(debug: bool = False) -> NewsAnalyzer:
    """ë‰´ìŠ¤ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _analyzer_instance
    if _analyzer_instance is None or _analyzer_instance.debug != debug:
        _analyzer_instance = NewsAnalyzer(debug=debug)
    return _analyzer_instance


def analyze_ticker_news(ticker: str, company_name: str, date: str, debug: bool = False) -> Dict[str, any]:
    """íŠ¹ì • ì¢…ëª©ì˜ ë‰´ìŠ¤ ë¶„ì„"""
    analyzer = get_news_analyzer(debug)
    news_list = analyzer.fetch_ticker_news(ticker, company_name, date)
    return analyzer.analyze_news_sentiment(news_list, ticker, company_name)
