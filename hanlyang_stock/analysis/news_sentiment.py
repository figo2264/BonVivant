"""
ë‰´ìŠ¤ ê°ì • ë¶„ì„ ê¸°ë°˜ ì£¼ê°€ ì˜ˆì¸¡ ëª¨ë“ˆ
News sentiment analysis based stock price prediction module
"""

import requests
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional
import pandas as pd
import json
from bs4 import BeautifulSoup
import os
from anthropic import Anthropic
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from webdriver_manager.chrome import ChromeDriverManager
import time
from pykrx import stock

# .env íŒŒì¼ ìˆ˜ë™ ë¡œë“œ
def load_env():
    env_path = Path(__file__).parent.parent.parent / '.env'
    if env_path.exists():
        with open(env_path, 'r') as f:
            for line in f:
                if line.strip() and not line.startswith('#'):
                    key, value = line.strip().split('=', 1)
                    os.environ[key] = value


class NewsAnalyzer:
    """ë‰´ìŠ¤ ê°ì • ë¶„ì„ í´ë˜ìŠ¤ - Claude API ì‚¬ìš©"""
    
    def __init__(self, api_key: Optional[str] = None, debug: bool = False):
        """
        Initialize NewsAnalyzer with Claude API
        
        Args:
            api_key: Anthropic API key
            debug: ë””ë²„ê·¸ ëª¨ë“œ
        """
        # .env íŒŒì¼ ë¡œë“œ
        load_env()
        
        self.api_key = api_key or os.getenv('ANTHROPIC_API_KEY')
        self.debug = debug
        
        if self.api_key:
            self.client = Anthropic(api_key=self.api_key)
        else:
            self.client = None
            print("âš ï¸ Claude API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‰´ìŠ¤ ë¶„ì„ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
        
        # ì¢…ëª©ë³„ íšŒì‚¬ëª… ìºì‹œ
        self.company_name_cache = {}
    
    def fetch_ticker_news(self, ticker: str, company_name: str, date: str) -> List[Dict[str, str]]:
        """
        íŠ¹ì • ì¢…ëª©ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ (ë§¤ì¼ê²½ì œ)
        
        Args:
            ticker: ì¢…ëª© ì½”ë“œ (ì˜ˆ: '005930')
            company_name: íšŒì‚¬ëª… (ì˜ˆ: 'ì‚¼ì„±ì „ì')
            date: ë‚ ì§œ (YYYY-MM-DD)
            
        Returns:
            List[Dict]: ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ [{title, date, url}, ...]
        """
        news_list = []
        
        # íšŒì‚¬ëª…ì´ ì¢…ëª©ì½”ë“œì™€ ê°™ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ì‹¤ì œ íšŒì‚¬ëª… ì¡°íšŒ
        if company_name == ticker or not company_name:
            # ìºì‹œ í™•ì¸
            if ticker in self.company_name_cache:
                company_name = self.company_name_cache[ticker]
                if self.debug:
                    print(f"  ğŸ¢ ìºì‹œì—ì„œ íšŒì‚¬ëª… ì¡°íšŒ: {ticker} â†’ {company_name}")
            else:
                try:
                    actual_company_name = stock.get_market_ticker_name(ticker)
                    if actual_company_name:
                        company_name = actual_company_name
                        self.company_name_cache[ticker] = company_name
                        print(f"  ğŸ¢ ì¢…ëª© ì½”ë“œ {ticker} â†’ íšŒì‚¬ëª…: {company_name}")
                except Exception as e:
                    print(f"  âš ï¸ íšŒì‚¬ëª… ì¡°íšŒ ì‹¤íŒ¨ ({ticker}): {e}")
                    # íšŒì‚¬ëª…ì„ ëª¨ë¥´ë©´ ì¢…ëª© ì½”ë“œë¡œë¼ë„ ê²€ìƒ‰ ì‹œë„
        
        # Selenium ë“œë¼ì´ë²„ ì„¤ì •
        serv = Service(ChromeDriverManager().install())
        chrome_options = webdriver.ChromeOptions()
        
        # ë””ë²„ê·¸ ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°ë§Œ headless
        if not self.debug:
            chrome_options.add_argument('--headless')
        
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        # User-Agent ì„¤ì • (ë´‡ìœ¼ë¡œ ì¸ì‹ë˜ëŠ” ê²ƒ ë°©ì§€)
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36')
        
        driver = webdriver.Chrome(service=serv, options=chrome_options)
        
        try:
            # ë§¤ì¼ê²½ì œ AI ê²€ìƒ‰ URL (ë” ê°„ë‹¨í•œ êµ¬ì¡°)
            url = f'https://www.mk.co.kr/aisearch?word={company_name}'
            
            print(f"  ğŸ“¡ ë§¤ì¼ê²½ì œ AI ê²€ìƒ‰ URL: {url}")
            driver.get(url)
            
            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            time.sleep(3)
            
            # ê²€ìƒ‰ ê²°ê³¼ ëŒ€ê¸° ë° íŒŒì‹±
            try:
                # í˜ì´ì§€ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸° - h3 íƒœê·¸ë‚˜ result í´ë˜ìŠ¤ê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€
                WebDriverWait(driver, 10).until(
                    lambda driver: driver.find_elements(By.CSS_SELECTOR, 'h3.news_ttl, [class*="result"], div[class*="news"]')
                )
                
                # ì¶”ê°€ ëŒ€ê¸° (ë™ì  ì»¨í…ì¸  ë¡œë”©)
                time.sleep(2)
                
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                
                # ë§¤ì¼ê²½ì œ AI ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ì°¾ê¸°
                news_items = []
                
                # ë°©ë²• 1: h3.news_ttl ì„ íƒìë¡œ ë‰´ìŠ¤ ì œëª© ì§ì ‘ ì°¾ê¸°
                news_titles = soup.select('h3.news_ttl')
                print(f"  ğŸ“Š h3.news_ttlë¡œ {len(news_titles)}ê°œ ë‰´ìŠ¤ ì œëª© ë°œê²¬")
                
                # ë°©ë²• 2: ë‰´ìŠ¤ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
                if not news_titles:
                    news_containers = soup.select('div[class*="news"]')
                    print(f"  ğŸ“Š div[class*='news']ë¡œ {len(news_containers)}ê°œ ì»¨í…Œì´ë„ˆ ë°œê²¬")
                    
                    # ê° ì»¨í…Œì´ë„ˆì—ì„œ ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ
                    for container in news_containers:
                        title_elem = container.find('h3') or container.find('h2') or container.find('h4')
                        if title_elem:
                            news_titles.append(title_elem)
                
                # ë°©ë²• 3: ê¸°ì‚¬ ë§í¬ê°€ ìˆëŠ” ëª¨ë“  í•­ëª© ì°¾ê¸°
                if not news_titles:
                    article_links = soup.select('a[href*="/news/"], a[href*="/article/"]')
                    print(f"  ğŸ“Š ê¸°ì‚¬ ë§í¬ë¡œ {len(article_links)}ê°œ ë°œê²¬")
                    
                    # ë§í¬ì—ì„œ ì œëª©ì´ ìˆëŠ” ê²ƒë§Œ ì¶”ì¶œ
                    for link in article_links:
                        if link.text.strip() and len(link.text.strip()) > 10:
                            news_titles.append(link)
                
                print(f"  ğŸ“Š ì´ {len(news_titles)}ê°œ ë‰´ìŠ¤ ì œëª© ìˆ˜ì§‘")
                
                # ë‰´ìŠ¤ ì •ë³´ ì¶”ì¶œ
                for idx, title_elem in enumerate(news_titles[:20]):  # ìµœê·¼ 20ê°œë§Œ ì¶”ì¶œ
                    try:
                        # ì œëª© ì¶”ì¶œ
                        title = title_elem.text.strip()
                        
                        # ì œëª©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ AI ë‹µë³€ ë“± ë¶ˆí•„ìš”í•œ ë‚´ìš© í•„í„°ë§
                        if not title or len(title) < 10:
                            continue
                        
                        # AI ê´€ë ¨ ì»¨í…ì¸  í•„í„°ë§
                        exclude_keywords = ['AI ë‹µë³€', 'ì°¸ê³ ìë£Œ', 'ê´€ë ¨ ì§ˆë¬¸', 'Powered by', 'perplexity']
                        if any(keyword in title for keyword in exclude_keywords):
                            continue
                        
                        # ì‹¤ì œ ë‰´ìŠ¤ ì œëª© íŒ¨í„´ í™•ì¸ (ë³´í†µ íšŒì‚¬ëª…ì´ë‚˜ ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨)
                        if company_name not in title and ticker not in title:
                            # ì œëª©ì— íšŒì‚¬ëª…ì´ë‚˜ ì¢…ëª©ì½”ë“œê°€ ì—†ìœ¼ë©´ ê´€ë ¨ì„± ë‚®ìŒ
                            # í•˜ì§€ë§Œ ì™„ì „íˆ ì œì™¸í•˜ì§€ëŠ” ì•ŠìŒ (ê´€ë ¨ ì—…ê³„ ë‰´ìŠ¤ì¼ ìˆ˜ ìˆìŒ)
                            pass
                        
                        # URL ì¶”ì¶œ - ì œëª© ìš”ì†Œë‚˜ ë¶€ëª¨ ìš”ì†Œì—ì„œ ë§í¬ ì°¾ê¸°
                        news_url = ''
                        
                        # ì œëª© ìš”ì†Œ ìì²´ê°€ ë§í¬ì¸ ê²½ìš°
                        if title_elem.name == 'a' and title_elem.get('href'):
                            news_url = title_elem['href']
                        else:
                            # ë¶€ëª¨ë‚˜ í˜•ì œ ìš”ì†Œì—ì„œ ë§í¬ ì°¾ê¸°
                            parent = title_elem.parent
                            while parent and parent.name != 'body':
                                link = parent.find('a', href=True)
                                if link and link.get('href'):
                                    news_url = link['href']
                                    break
                                parent = parent.parent
                        
                        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                        if news_url and not news_url.startswith('http'):
                            if news_url.startswith('//'):
                                news_url = 'https:' + news_url
                            else:
                                news_url = 'https://www.mk.co.kr' + news_url
                        
                        # ë‚ ì§œ ì¶”ì¶œ - í˜„ì¬ ë‚ ì§œë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
                        news_date = date
                        
                        # ë¶€ëª¨ ìš”ì†Œì—ì„œ ë‚ ì§œ ì •ë³´ ì°¾ê¸°
                        parent = title_elem.parent
                        while parent and parent.name != 'body':
                            date_text = parent.find(string=lambda x: x and any(year in str(x) for year in ['2023', '2024', '2025']))
                            if date_text:
                                date_str = str(date_text).strip()
                                # ë‚ ì§œ í˜•ì‹ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
                                if len(date_str) < 20 and ('.' in date_str or '-' in date_str or 'ë…„' in date_str):
                                    news_date = date_str
                                    break
                            parent = parent.parent
                        
                        news_list.append({
                            'title': title,
                            'date': news_date,
                            'url': news_url
                        })
                        
                        print(f"  âœ… ë‰´ìŠ¤ {idx+1}: {title[:50]}...")
                        
                    except Exception as e:
                        print(f"  âš ï¸ ë‰´ìŠ¤ í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                
                # ë” ë§ì€ ë‰´ìŠ¤ ë¡œë“œ ì‹œë„ (ë¬´í•œ ìŠ¤í¬ë¡¤ì´ë‚˜ ë”ë³´ê¸° ë²„íŠ¼)
                if len(news_list) < 10:
                    print(f"  ğŸ“Œ ë‰´ìŠ¤ê°€ {len(news_list)}ê°œë¿ì´ë¯€ë¡œ ì¶”ê°€ ë¡œë“œ ì‹œë„...")
                    
                    # 1. ìŠ¤í¬ë¡¤ì„ ë‚´ë ¤ì„œ ë” ë§ì€ ë‰´ìŠ¤ ë¡œë“œ ì‹œë„
                    for _ in range(3):  # 3ë²ˆ ìŠ¤í¬ë¡¤
                        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                        time.sleep(1.5)
                    
                    # 2. ë”ë³´ê¸° ë²„íŠ¼ ì°¾ê¸° - ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
                    try:
                        more_button_selectors = [
                            'button:contains("ë”ë³´ê¸°")',
                            'a:contains("ë”ë³´ê¸°")',
                            'button[class*="more"]',
                            'a[class*="more"]',
                            'button.btn_more',
                            'a.btn_more',
                            'div[class*="more"] button',
                            'div[class*="more"] a'
                        ]
                        
                        for selector in more_button_selectors:
                            try:
                                # JavaScriptë¡œ ìš”ì†Œ ì°¾ê¸° (contains ì„ íƒì ì§€ì›)
                                if ':contains(' in selector:
                                    text = selector.split('"')[1]
                                    element_type = selector.split(':')[0]
                                    buttons = driver.find_elements(By.TAG_NAME, element_type)
                                    for button in buttons:
                                        if text in button.text and button.is_displayed():
                                            button.click()
                                            print(f"  âœ… ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì„±ê³µ: {button.text}")
                                            time.sleep(2)
                                            break
                                else:
                                    # CSS ì„ íƒìë¡œ ì°¾ê¸°
                                    buttons = driver.find_elements(By.CSS_SELECTOR, selector)
                                    for button in buttons:
                                        if button.is_displayed():
                                            button.click()
                                            print(f"  âœ… ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
                                            time.sleep(2)
                                            break
                            except:
                                continue
                    except Exception as e:
                        print(f"  âš ï¸ ë”ë³´ê¸° ë²„íŠ¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    
                    # 3. í˜ì´ì§€ ì¬íŒŒì‹±í•˜ì—¬ ì¶”ê°€ ë‰´ìŠ¤ ìˆ˜ì§‘
                    soup = BeautifulSoup(driver.page_source, 'html.parser')
                    
                    # ì¶”ê°€ ë‰´ìŠ¤ ì œëª© ì°¾ê¸°
                    additional_titles = soup.select('h3.news_ttl')
                    for title_elem in additional_titles[len(news_list):]:  # ì´ë¯¸ ìˆ˜ì§‘í•œ ê²ƒ ì´í›„ë¶€í„°
                        try:
                            title = title_elem.text.strip()
                            if not title or len(title) < 10:
                                continue
                            
                            # URLê³¼ ë‚ ì§œ ì¶”ì¶œ (ìœ„ì™€ ë™ì¼í•œ ë¡œì§)
                            news_url = ''
                            if title_elem.name == 'a' and title_elem.get('href'):
                                news_url = title_elem['href']
                            else:
                                parent = title_elem.parent
                                while parent and parent.name != 'body':
                                    link = parent.find('a', href=True)
                                    if link and link.get('href'):
                                        news_url = link['href']
                                        break
                                    parent = parent.parent
                            
                            if news_url and not news_url.startswith('http'):
                                if news_url.startswith('//'):
                                    news_url = 'https:' + news_url
                                else:
                                    news_url = 'https://www.mk.co.kr' + news_url
                            
                            news_list.append({
                                'title': title,
                                'date': date,
                                'url': news_url
                            })
                        except Exception as e:
                            continue
                
            except Exception as e:
                print(f"  âš ï¸ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                # ë””ë²„ê¹…ì„ ìœ„í•´ í˜ì´ì§€ ì†ŒìŠ¤ ì¼ë¶€ ì¶œë ¥
                print(f"  ğŸ“„ í˜ì´ì§€ ì œëª©: {driver.title}")
                print(f"  ğŸ“„ URL: {driver.current_url}")
                
                # í˜ì´ì§€ êµ¬ì¡° ë””ë²„ê¹…
                try:
                    soup = BeautifulSoup(driver.page_source, 'html.parser')
                    
                    # AI ê´€ë ¨ ìš”ì†Œ í™•ì¸
                    ai_elements = soup.select('[class*="ai"], [id*="ai"]')
                    print(f"  ğŸ“„ AI ê´€ë ¨ ìš”ì†Œ: {len(ai_elements)}ê°œ")
                    
                    # ë‰´ìŠ¤ ê´€ë ¨ ìš”ì†Œ í™•ì¸
                    news_elements = soup.select('h3.news_ttl, div[class*="news"], [class*="result"]')
                    print(f"  ğŸ“„ ë‰´ìŠ¤ ê´€ë ¨ ìš”ì†Œ: {len(news_elements)}ê°œ")
                    
                    # ì²« ë²ˆì§¸ h3 ìš”ì†Œë“¤ ì¶œë ¥
                    h3_elements = soup.find_all('h3')[:5]
                    print(f"  ğŸ“„ ì²« 5ê°œ h3 ìš”ì†Œ:")
                    for i, h3 in enumerate(h3_elements):
                        print(f"    {i+1}. {h3.text.strip()[:50]}...")
                except:
                    pass
                
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜ ({ticker}): {e}")
            
        finally:
            driver.quit()
        
        print(f"  âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(news_list)}ê°œ ë‰´ìŠ¤")
        return news_list
    
    def analyze_news_sentiment(self, news_list: List[Dict], ticker: str, company_name: str, 
                               days: List[int] = [1, 5, 10, 20]) -> Dict[str, any]:
        """
        Claude APIë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ê°ì • ë¶„ì„ ë° ì£¼ê°€ ì˜ˆì¸¡
        
        Args:
            news_list: ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
            ticker: ì¢…ëª© ì½”ë“œ
            company_name: íšŒì‚¬ëª…
            days: ì˜ˆì¸¡í•  ì¼ìˆ˜ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict: ì˜ˆì¸¡ ê²°ê³¼
        """
        # ë””ë²„ê¹…: ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ìƒíƒœ í™•ì¸
        if self.debug:
            print(f"  [DEBUG] ë‰´ìŠ¤ ê°œìˆ˜: {len(news_list) if news_list else 0}")
            print(f"  [DEBUG] Claude í´ë¼ì´ì–¸íŠ¸: {'ìˆìŒ' if self.client else 'ì—†ìŒ'}")
        
        if not self.client or not news_list:
            if self.debug:
                print(f"  [DEBUG] ê¸°ë³¸ê°’ ë°˜í™˜ - í´ë¼ì´ì–¸íŠ¸: {bool(self.client)}, ë‰´ìŠ¤: {bool(news_list)}")
            return self._get_default_predictions(days)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        news_titles = "\n".join([f"{i+1}. {news['title']}" for i, news in enumerate(news_list[:5])])
        
        prompt = f"""ë‹¤ìŒì€ {company_name}({ticker}) ê´€ë ¨ ìµœê·¼ ë‰´ìŠ¤ ì œëª©ë“¤ì…ë‹ˆë‹¤:

{news_titles}

ìœ„ ë‰´ìŠ¤ë“¤ì„ ë¶„ì„í•˜ì—¬ {company_name} ì£¼ê°€ê°€ í–¥í›„ {days[0]}, {days[1]}, {days[2]}, {days[3]}ì¼ í›„ì— 
ê°ê° ìƒìŠ¹í•  í™•ë¥ ì„ 0-100 ì‚¬ì´ì˜ ìˆ«ìë¡œ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”:
- {days[0]}ì¼ í›„ ìƒìŠ¹ í™•ë¥ : XX%
- {days[1]}ì¼ í›„ ìƒìŠ¹ í™•ë¥ : XX%
- {days[2]}ì¼ í›„ ìƒìŠ¹ í™•ë¥ : XX%
- {days[3]}ì¼ í›„ ìƒìŠ¹ í™•ë¥ : XX%
- ì¢…í•© ê°ì •: ê¸ì •/ì¤‘ë¦½/ë¶€ì •
- ì£¼ìš” ì´ìœ : (í•œ ì¤„ë¡œ ì„¤ëª…)"""
        
        try:
            # Claude API í˜¸ì¶œ
            if self.debug:
                print(f"  [DEBUG] Claude API í˜¸ì¶œ ì‹œì‘...")
            
            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=300,
                temperature=0.0,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            # ë””ë²„ê¹…: ì‘ë‹µ ë‚´ìš© í™•ì¸
            response_text = response.content[0].text
            if self.debug:
                print(f"  [DEBUG] Claude ì‘ë‹µ ê¸¸ì´: {len(response_text)}ì")
                print(f"  [DEBUG] Claude ì‘ë‹µ ì²« 100ì: {response_text[:100]}...")
            
            # ì‘ë‹µ íŒŒì‹±
            return self._parse_claude_response(response_text, days)
        
        except Exception as e:
            print(f"âŒ Claude API ì˜¤ë¥˜: {e}")
            return self._get_default_predictions(days)
    
    def _parse_claude_response(self, response_text: str, days: List[int]) -> Dict[str, any]:
        """Claude ì‘ë‹µ íŒŒì‹±"""
        predictions = {}
        
        # ë””ë²„ê¹…: íŒŒì‹± ì‹œì‘
        if self.debug:
            print(f"  [DEBUG] ì‘ë‹µ íŒŒì‹± ì‹œì‘...")
        
        try:
            lines = response_text.strip().split('\n')
            if self.debug:
                print(f"  [DEBUG] ì‘ë‹µ ë¼ì¸ ìˆ˜: {len(lines)}")
            
            # ê° ì¼ìˆ˜ë³„ í™•ë¥  ì¶”ì¶œ
            for i, day in enumerate(days):
                for line in lines:
                    if f"{day}ì¼ í›„" in line:
                        prob = float(line.split(':')[1].strip().replace('%', ''))
                        predictions[f'prob_{day}'] = prob / 100.0
                        if self.debug:
                            print(f"  [DEBUG] {day}ì¼ í™•ë¥  íŒŒì‹±: {prob}%")
                        break
            
            # ì¢…í•© ê°ì • ì¶”ì¶œ
            for line in lines:
                if 'ì¢…í•© ê°ì •:' in line:
                    sentiment = line.split(':')[1].strip()
                    predictions['sentiment'] = sentiment
                    if self.debug:
                        print(f"  [DEBUG] ê°ì • íŒŒì‹±: {sentiment}")
                elif 'ì£¼ìš” ì´ìœ :' in line:
                    reason = line.split(':')[1].strip()
                    predictions['reason'] = reason
            
            # í‰ê·  í™•ë¥  ê³„ì‚°
            prob_values = [v for k, v in predictions.items() if k.startswith('prob_')]
            predictions['avg_confidence'] = sum(prob_values) / len(prob_values) if prob_values else 0.5
            if self.debug:
                print(f"  [DEBUG] íŒŒì‹±ëœ í™•ë¥  ê°œìˆ˜: {len(prob_values)}")
                print(f"  [DEBUG] í‰ê·  ì‹ ë¢°ë„: {predictions['avg_confidence'] * 100:.1f}%")
            
        except Exception as e:
            print(f"âš ï¸ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return self._get_default_predictions(days)
        
        return predictions
    
    def _get_default_predictions(self, days: List[int]) -> Dict[str, any]:
        """ê¸°ë³¸ ì˜ˆì¸¡ê°’ ë°˜í™˜"""
        predictions = {
            'avg_confidence': 0.5,
            'sentiment': 'ì¤‘ë¦½',
            'reason': 'ë‰´ìŠ¤ ë¶„ì„ ë¶ˆê°€'
        }
        
        for day in days:
            predictions[f'prob_{day}'] = 0.5
        
        return predictions
    
    def find_optimal_parameters(self, historical_data: pd.DataFrame, 
                                days_list: List[int] = [1, 5, 10, 20],
                                threshold_list: List[float] = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8]) -> Tuple[int, float]:
        """
        ê³¼ê±° ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ ë³´ìœ  ê¸°ê°„(n)ê³¼ ë§¤ìˆ˜ ê¸°ì¤€ í™•ë¥ (threshold) ì°¾ê¸°
        
        Args:
            historical_data: ê³¼ê±° ë‰´ìŠ¤ ë° ì£¼ê°€ ë°ì´í„°
            days_list: í…ŒìŠ¤íŠ¸í•  ë³´ìœ  ê¸°ê°„ ë¦¬ìŠ¤íŠ¸
            threshold_list: í…ŒìŠ¤íŠ¸í•  ë§¤ìˆ˜ ê¸°ì¤€ í™•ë¥  ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Tuple[int, float]: (ìµœì  ë³´ìœ  ê¸°ê°„, ìµœì  ë§¤ìˆ˜ ê¸°ì¤€ í™•ë¥ )
        """
        best_return = -float('inf')
        best_n = days_list[0]
        best_threshold = threshold_list[0]
        
        for n in days_list:
            for threshold in threshold_list:
                # ë°±í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
                total_return = self._backtest_strategy(historical_data, n, threshold)
                
                if total_return > best_return:
                    best_return = total_return
                    best_n = n
                    best_threshold = threshold
        
        print(f"ğŸ¯ ìµœì  íŒŒë¼ë¯¸í„°: ë³´ìœ ê¸°ê°„={best_n}ì¼, ë§¤ìˆ˜ê¸°ì¤€={best_threshold:.2f}, ìˆ˜ìµë¥ ={best_return:.2%}")
        
        return best_n, best_threshold
    
    def _backtest_strategy(self, data: pd.DataFrame, n: int, threshold: float) -> float:
        """ê°„ë‹¨í•œ ë°±í…ŒìŠ¤íŠ¸ êµ¬í˜„"""
        initial_capital = 10_000_000
        capital = initial_capital
        position = 0
        
        # ì‹¤ì œ ë°±í…ŒìŠ¤íŠ¸ ë¡œì§ì€ ë” ë³µì¡í•˜ê²Œ êµ¬í˜„ í•„ìš”
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë§Œ ì œê³µ
        
        return (capital - initial_capital) / initial_capital
    
    def analyze_ticker_news(self, ticker: str, company_name: str, date: str) -> Dict[str, any]:
        """
        íŠ¹ì • ì¢…ëª©ì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„
        
        Args:
            ticker: ì¢…ëª© ì½”ë“œ
            company_name: íšŒì‚¬ëª…
            date: ë‚ ì§œ (YYYY-MM-DD)
            
        Returns:
            Dict: ë¶„ì„ ê²°ê³¼
        """
        # ë‰´ìŠ¤ ìˆ˜ì§‘
        news_list = self.fetch_ticker_news(ticker, company_name, date)
        
        # ê°ì • ë¶„ì„ ë° ì˜ˆì¸¡
        return self.analyze_news_sentiment(news_list, ticker, company_name)


# í¸ì˜ í•¨ìˆ˜ë“¤
_analyzer_instance = None

def get_news_analyzer(debug: bool = False) -> NewsAnalyzer:
    """ë‰´ìŠ¤ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _analyzer_instance
    if _analyzer_instance is None or _analyzer_instance.debug != debug:
        _analyzer_instance = NewsAnalyzer(debug=debug)
    return _analyzer_instance


def analyze_ticker_news(ticker: str, company_name: str, date: str, debug: bool = False) -> Dict[str, any]:
    """íŠ¹ì • ì¢…ëª©ì˜ ë‰´ìŠ¤ ë¶„ì„"""
    analyzer = get_news_analyzer(debug)
    news_list = analyzer.fetch_ticker_news(ticker, company_name, date)
    return analyzer.analyze_news_sentiment(news_list, ticker, company_name)
