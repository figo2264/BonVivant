"""
ë„¤ì´ë²„ ì¦ê¶Œ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
Naver Finance news crawler implementation
"""

from typing import List
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import tempfile
import os
import shutil
from datetime import datetime

from .news_crawler_base import NewsCrawlerBase, NewsItem


class NaverFinanceCrawler(NewsCrawlerBase):
    """ë„¤ì´ë²„ ì¦ê¶Œ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._temp_dir = None
    
    def _get_quality_sources(self) -> List[str]:
        """ê²½ì œ ì „ë¬¸ ë§¤ì²´ ë¦¬ìŠ¤íŠ¸"""
        return [
            'í•œêµ­ê²½ì œ', 'í•œê²½', 'ì—°í•©ì¸í¬ë§¥ìŠ¤', 'ì¸í¬ë§¥ìŠ¤',
            'ë§¤ì¼ê²½ì œ', 'ë§¤ê²½', 'ì„œìš¸ê²½ì œ', 'ì´ë°ì¼ë¦¬',
            'ë¨¸ë‹ˆíˆ¬ë°ì´', 'íŒŒì´ë‚¸ì…œë‰´ìŠ¤', 'ì•„ì‹œì•„ê²½ì œ',
            'í—¤ëŸ´ë“œê²½ì œ', 'ì¡°ì„ ë¹„ì¦ˆ', 'ë‰´ìŠ¤1', 'ë‰´ì‹œìŠ¤'
        ]
    
    def fetch_news(self, ticker: str, company_name: str, date: str, 
                   max_items: int = 10) -> List[NewsItem]:
        """ë„¤ì´ë²„ ì¦ê¶Œì—ì„œ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        print(f"ğŸ“¡ ë„¤ì´ë²„ ì¦ê¶Œì—ì„œ {ticker} ({company_name}) ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        
        news_list = []
        driver = None
        
        try:
            driver = self._setup_driver()
            url = f"https://finance.naver.com/item/news.naver?code={ticker}"
            
            print(f"  ğŸ“„ í˜ì´ì§€ ì ‘ì†: {url}")
            driver.get(url)
            
            # iframe ì²˜ë¦¬
            self._switch_to_news_frame(driver)
            
            # ë‰´ìŠ¤ íŒŒì‹±
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            news_list = self._parse_news_items(soup, date)
            
            # í’ˆì§ˆ í•„í„°ë§
            filtered_news = self.filter_by_quality(news_list)
            
            # ë‚ ì§œìˆœ ì •ë ¬
            filtered_news.sort(key=lambda x: self._parse_news_date(x.date), reverse=True)
            
            # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
            final_news = filtered_news[:max_items]
            
            print(f"  ğŸ“° ìµœì¢… {len(final_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì „ì²´ {len(news_list)}ê°œ ì¤‘)")
            
            # ì†ŒìŠ¤ë³„ í†µê³„
            if final_news and self.debug:
                self._print_source_stats(final_news)
            
            return final_news
            
        except Exception as e:
            print(f"âŒ ë„¤ì´ë²„ ì¦ê¶Œ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            if self.debug:
                import traceback
                traceback.print_exc()
            return []
            
        finally:
            if driver:
                try:
                    # Chrome ì¢…ë£Œ
                    driver.quit()
                    
                    # Chrome user-data-dir ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
                    # chrome_optionsì—ì„œ user-data-dir ì°¾ê¸°
                    if hasattr(self, '_temp_dir') and os.path.exists(self._temp_dir):
                        shutil.rmtree(self._temp_dir, ignore_errors=True)
                except Exception as e:
                    print(f"âš ï¸ ë“œë¼ì´ë²„ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _setup_driver(self) -> webdriver.Chrome:
        """Selenium ë“œë¼ì´ë²„ ì„¤ì •"""
        
        chrome_options = webdriver.ChromeOptions()
        if not self.debug:
            chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        # ê³ ìœ í•œ user-data-dir ì„¤ì •ìœ¼ë¡œ ì„¸ì…˜ ì¶©ëŒ ë°©ì§€
        self._temp_dir = tempfile.mkdtemp()
        chrome_options.add_argument(f'--user-data-dir={self._temp_dir}')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        
        # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™” (ì†ë„ í–¥ìƒ)
        prefs = {"profile.managed_default_content_settings.images": 2}
        chrome_options.add_experimental_option("prefs", prefs)
        
        serv = Service(ChromeDriverManager().install())
        return webdriver.Chrome(service=serv, options=chrome_options)
    
    def _switch_to_news_frame(self, driver):
        """ë‰´ìŠ¤ iframeìœ¼ë¡œ ì „í™˜"""
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.ID, "news_frame"))
            )
            news_iframe = driver.find_element(By.ID, "news_frame")
            driver.switch_to.frame(news_iframe)
            print("  âœ… ë‰´ìŠ¤ í”„ë ˆì„ ì§„ì… ì„±ê³µ")
            
            # ë‰´ìŠ¤ í…Œì´ë¸” ëŒ€ê¸°
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "table.type5"))
            )
        except Exception as e:
            print(f"  âš ï¸ iframe ì „í™˜ ì‹¤íŒ¨: {e}")
    
    def _parse_news_items(self, soup: BeautifulSoup, date: str) -> List[NewsItem]:
        """ë‰´ìŠ¤ ì•„ì´í…œ íŒŒì‹±"""
        news_items = []
        news_table = soup.find('table', class_='type5')
        
        if not news_table:
            print("  âš ï¸ ë‰´ìŠ¤ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return news_items
        
        rows = news_table.find_all('tr')
        print(f"  ğŸ“Š {len(rows)}ê°œ í–‰ ë°œê²¬")
        
        for row in rows:
            if row.find('th'):  # í—¤ë” í–‰ ìŠ¤í‚µ
                continue
            
            title_cell = row.find('td', class_='title')
            if not title_cell:
                continue
            
            link_elem = title_cell.find('a')
            if not link_elem:
                continue
            
            # ì •ë³´ ì¶”ì¶œ
            title = link_elem.text.strip()
            news_url = link_elem.get('href', '')
            
            # URL ì •ê·œí™”
            if news_url and not news_url.startswith('http'):
                if news_url.startswith('//'):
                    news_url = 'https:' + news_url
                else:
                    news_url = 'https://finance.naver.com' + news_url
            
            # ì–¸ë¡ ì‚¬
            info_cell = row.find('td', class_='info')
            source = info_cell.text.strip() if info_cell else 'ì•Œ ìˆ˜ ì—†ìŒ'
            
            # ë‚ ì§œ
            date_cell = row.find('td', class_='date')
            news_date = date_cell.text.strip() if date_cell else date
            
            news_items.append(NewsItem(
                title=title,
                date=news_date,
                url=news_url,
                source=source
            ))
        
        return news_items
    
    def _parse_news_date(self, date_str: str) -> datetime:
        """ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜"""
        try:
            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
            if 'ì‹œê°„' in date_str or 'ë¶„' in date_str:
                return datetime.now()
            elif '.' in date_str:
                date_parts = date_str.split()[0].split('.')
                if len(date_parts[0]) == 2:
                    date_str = '20' + date_str
                return datetime.strptime(date_str.split()[0], '%Y.%m.%d')
            else:
                return datetime.now()
        except:
            return datetime.now()
    
    def _print_source_stats(self, news_items: List[NewsItem]):
        """ì†ŒìŠ¤ë³„ í†µê³„ ì¶œë ¥"""
        source_stats = {}
        for news in news_items:
            src = news.source
            source_stats[src] = source_stats.get(src, 0) + 1
        
        print(f"  ğŸ“Š ì†ŒìŠ¤ë³„ ë¶„í¬:")
        for src, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True):
            print(f"     - {src}: {count}ê°œ")
