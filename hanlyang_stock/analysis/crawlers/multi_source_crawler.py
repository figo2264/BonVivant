"""
ë©€í‹° ì†ŒìŠ¤ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
Crawler that aggregates news from multiple sources
"""

from typing import List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

from .news_crawler_base import NewsItem, NewsSource
from .news_crawler_factory import NewsCrawlerFactory


class MultiSourceCrawler:
    """ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í†µí•© í¬ë¡¤ëŸ¬"""
    
    def __init__(self, sources: Optional[List[str]] = None, debug: bool = False,
                 parallel: bool = True, max_workers: int = 3):
        """
        Initialize multi-source crawler
        
        Args:
            sources: ì‚¬ìš©í•  ë‰´ìŠ¤ ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            debug: ë””ë²„ê·¸ ëª¨ë“œ
            parallel: ë³‘ë ¬ ì²˜ë¦¬ ì—¬ë¶€
            max_workers: ìµœëŒ€ ì›Œì»¤ ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ ì‹œ)
        """
        self.debug = debug
        self.parallel = parallel
        self.max_workers = max_workers
        
        # ê¸°ë³¸ ì†ŒìŠ¤ ì„¤ì •
        if sources is None:
            sources = [NewsSource.NAVER_FINANCE]
        
        self.sources = sources
        self._validate_sources()
        
        # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.crawlers = {
            source: NewsCrawlerFactory.create_crawler(source, debug)
            for source in self.sources
        }
        
        print(f"ğŸ“¡ ë©€í‹° ì†ŒìŠ¤ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”: {len(self.crawlers)}ê°œ ì†ŒìŠ¤")
        if debug:
            print(f"   - ì†ŒìŠ¤: {', '.join(self.sources)}")
            print(f"   - ë³‘ë ¬ ì²˜ë¦¬: {'í™œì„±í™”' if parallel else 'ë¹„í™œì„±í™”'}")
    
    def _validate_sources(self):
        """ì†ŒìŠ¤ ìœ íš¨ì„± ê²€ì¦"""
        available_sources = NewsCrawlerFactory.get_available_sources()
        invalid_sources = [s for s in self.sources if s not in available_sources]
        
        if invalid_sources:
            raise ValueError(
                f"Invalid sources: {', '.join(invalid_sources)}. "
                f"Available sources: {', '.join(available_sources)}"
            )
    
    def fetch_news(self, ticker: str, company_name: str, date: str,
                   max_items: int = 10) -> List[NewsItem]:
        """
        ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° í†µí•©
        
        Args:
            ticker: ì¢…ëª© ì½”ë“œ
            company_name: íšŒì‚¬ëª…
            date: ê¸°ì¤€ ë‚ ì§œ (YYYY-MM-DD)
            max_items: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
            
        Returns:
            List[NewsItem]: í†µí•©ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\nğŸ” {ticker} ({company_name}) ë‰´ìŠ¤ í†µí•© ìˆ˜ì§‘ ì‹œì‘...")
        start_time = time.time()
        
        if self.parallel and len(self.crawlers) > 1:
            all_news = self._fetch_parallel(ticker, company_name, date, max_items)
        else:
            all_news = self._fetch_sequential(ticker, company_name, date, max_items)
        
        # ì¤‘ë³µ ì œê±°
        unique_news = self._remove_duplicates(all_news)
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
        unique_news.sort(key=lambda x: x.date, reverse=True)
        
        # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        final_news = unique_news[:max_items]
        
        elapsed_time = time.time() - start_time
        print(f"\nâœ… í†µí•© ìˆ˜ì§‘ ì™„ë£Œ: {len(final_news)}ê°œ ë‰´ìŠ¤ "
              f"(ì „ì²´ {len(all_news)}ê°œ, ì¤‘ë³µ {len(all_news) - len(unique_news)}ê°œ ì œê±°)")
        print(f"â±ï¸ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        
        # ì†ŒìŠ¤ë³„ í†µê³„
        if final_news:
            self._print_source_distribution(final_news)
        
        return final_news
    
    def _fetch_sequential(self, ticker: str, company_name: str, date: str,
                          max_items: int) -> List[NewsItem]:
        """ìˆœì°¨ì ìœ¼ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        all_news = []
        
        for source, crawler in self.crawlers.items():
            try:
                news_items = crawler.fetch_news(ticker, company_name, date, max_items)
                all_news.extend(news_items)
                print(f"  âœ… {source}: {len(news_items)}ê°œ ìˆ˜ì§‘")
            except Exception as e:
                print(f"  âš ï¸ {source} ì˜¤ë¥˜: {e}")
                if self.debug:
                    import traceback
                    traceback.print_exc()
        
        return all_news
    
    def _fetch_parallel(self, ticker: str, company_name: str, date: str,
                        max_items: int) -> List[NewsItem]:
        """ë³‘ë ¬ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        all_news = []
        
        with ThreadPoolExecutor(max_workers=min(self.max_workers, len(self.crawlers))) as executor:
            # í¬ë¡¤ë§ ì‘ì—… ì œì¶œ
            future_to_source = {
                executor.submit(
                    crawler.fetch_news, ticker, company_name, date, max_items
                ): source
                for source, crawler in self.crawlers.items()
            }
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_source):
                source = future_to_source[future]
                try:
                    news_items = future.result()
                    all_news.extend(news_items)
                    print(f"  âœ… {source}: {len(news_items)}ê°œ ìˆ˜ì§‘")
                except Exception as e:
                    print(f"  âš ï¸ {source} ì˜¤ë¥˜: {e}")
                    if self.debug:
                        import traceback
                        traceback.print_exc()
        
        return all_news
    
    def _remove_duplicates(self, news_items: List[NewsItem]) -> List[NewsItem]:
        """ì¤‘ë³µ ë‰´ìŠ¤ ì œê±° (ì œëª© ê¸°ë°˜)"""
        seen_titles = set()
        unique_items = []
        duplicate_count = 0
        
        for item in news_items:
            # ì œëª©ì˜ ì• 50ìë¥¼ í‚¤ë¡œ ì‚¬ìš©
            title_key = item.title[:50].lower().strip()
            
            if title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_items.append(item)
            else:
                duplicate_count += 1
                if self.debug:
                    print(f"  ğŸ” ì¤‘ë³µ ì œê±°: {item.title[:40]}...")
        
        if duplicate_count > 0:
            print(f"  ğŸ”„ {duplicate_count}ê°œ ì¤‘ë³µ ë‰´ìŠ¤ ì œê±°")
        
        return unique_items
    
    def _print_source_distribution(self, news_items: List[NewsItem]):
        """ì†ŒìŠ¤ë³„ ë‰´ìŠ¤ ë¶„í¬ ì¶œë ¥"""
        source_stats = {}
        
        for item in news_items:
            # í¬ë¡¤ëŸ¬ë³„ë¡œ ì†ŒìŠ¤ ê·¸ë£¹í™”
            crawler_source = None
            for source in self.sources:
                if source == NewsSource.NAVER_FINANCE:
                    # ë„¤ì´ë²„ ì¦ê¶Œ í¬ë¡¤ëŸ¬ì˜ ë‰´ìŠ¤ë“¤
                    if item.source != 'ë§¤ì¼ê²½ì œ':  # ë§¤ì¼ê²½ì œê°€ ì•„ë‹Œ ê²ƒë“¤
                        crawler_source = source
                        break
                elif source == NewsSource.MK_ECONOMY:
                    # ë§¤ì¼ê²½ì œ í¬ë¡¤ëŸ¬ì˜ ë‰´ìŠ¤ë“¤
                    if item.source == 'ë§¤ì¼ê²½ì œ':
                        crawler_source = source
                        break
            
            if crawler_source:
                if crawler_source not in source_stats:
                    source_stats[crawler_source] = {}
                
                # ì–¸ë¡ ì‚¬ë³„ ì¹´ìš´íŠ¸
                news_source = item.source
                source_stats[crawler_source][news_source] = \
                    source_stats[crawler_source].get(news_source, 0) + 1
        
        print(f"\n  ğŸ“Š ì†ŒìŠ¤ë³„ ë‰´ìŠ¤ ë¶„í¬:")
        for crawler, sources in source_stats.items():
            total = sum(sources.values())
            print(f"     [{crawler}] ì´ {total}ê°œ")
            for src, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
                print(f"       - {src}: {count}ê°œ")
    
    def add_source(self, source: str):
        """ìƒˆë¡œìš´ ì†ŒìŠ¤ ì¶”ê°€"""
        if source not in self.sources:
            self.sources.append(source)
            self.crawlers[source] = NewsCrawlerFactory.create_crawler(source, self.debug)
            print(f"âœ… ì†ŒìŠ¤ ì¶”ê°€: {source}")
        else:
            print(f"âš ï¸ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì†ŒìŠ¤: {source}")
    
    def remove_source(self, source: str):
        """ì†ŒìŠ¤ ì œê±°"""
        if source in self.sources:
            self.sources.remove(source)
            del self.crawlers[source]
            print(f"âœ… ì†ŒìŠ¤ ì œê±°: {source}")
        else:
            print(f"âš ï¸ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤: {source}")
