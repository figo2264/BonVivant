"""
ë§¤ì¼ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
Maeil Kyungjae news crawler implementation
"""

from typing import List
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import random

from .news_crawler_base import NewsCrawlerBase, NewsItem
from pykrx import stock


class MKEconomyCrawler(NewsCrawlerBase):
    """ë§¤ì¼ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, debug: bool = False):
        super().__init__(debug)
        self.company_name_cache = {}
    
    def _get_quality_sources(self) -> List[str]:
        """ë§¤ì¼ê²½ì œëŠ” ìì²´ í•„í„°ë§ ë¶ˆí•„ìš”"""
        return []
    
    def fetch_news(self, ticker: str, company_name: str, date: str,
                   max_items: int = 10) -> List[NewsItem]:
        """ë§¤ì¼ê²½ì œì—ì„œ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        print(f"ğŸ“¡ ë§¤ì¼ê²½ì œì—ì„œ {ticker} ({company_name}) ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        
        # íšŒì‚¬ëª… í™•ì¸
        if company_name == ticker or not company_name:
            company_name = self._get_company_name(ticker)
        
        news_list = []
        driver = None
        
        try:
            driver = self._setup_driver()
            
            # ë§¤ì¼ê²½ì œ AI ê²€ìƒ‰ URL
            url = f'https://www.mk.co.kr/aisearch?word={company_name}'
            print(f"  ğŸ“¡ ë§¤ì¼ê²½ì œ AI ê²€ìƒ‰ URL: {url}")
            
            driver.get(url)
            
            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            time.sleep(3)
            
            # ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±
            news_list = self._parse_search_results(driver, date, max_items)
            
            # ìŠ¤í¬ë¡¤ ë° ë”ë³´ê¸° ì‹œë„
            if len(news_list) < max_items:
                news_list.extend(self._load_more_news(driver, date, max_items - len(news_list)))
            
            # ì¤‘ë³µ ì œê±°
            unique_news = self._remove_duplicates(news_list)
            
            print(f"  âœ… ìµœì¢… {len(unique_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
            return unique_news[:max_items]
            
        except Exception as e:
            print(f"âŒ ë§¤ì¼ê²½ì œ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            if self.debug:
                import traceback
                traceback.print_exc()
            return []
            
        finally:
            if driver:
                driver.quit()
    
    def _get_company_name(self, ticker: str) -> str:
        """ì¢…ëª© ì½”ë“œë¡œ íšŒì‚¬ëª… ì¡°íšŒ"""
        if ticker in self.company_name_cache:
            return self.company_name_cache[ticker]
        
        try:
            actual_company_name = stock.get_market_ticker_name(ticker)
            if actual_company_name:
                self.company_name_cache[ticker] = actual_company_name
                print(f"  ğŸ¢ ì¢…ëª© ì½”ë“œ {ticker} â†’ íšŒì‚¬ëª…: {actual_company_name}")
                return actual_company_name
        except Exception as e:
            print(f"  âš ï¸ íšŒì‚¬ëª… ì¡°íšŒ ì‹¤íŒ¨ ({ticker}): {e}")
        
        return ticker
    
    def _setup_driver(self) -> webdriver.Chrome:
        """Selenium ë“œë¼ì´ë²„ ì„¤ì •"""
        chrome_options = webdriver.ChromeOptions()
        if not self.debug:
            chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        serv = Service(ChromeDriverManager().install())
        return webdriver.Chrome(service=serv, options=chrome_options)
    
    def _parse_search_results(self, driver, date: str, max_items: int) -> List[NewsItem]:
        """ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±"""
        news_list = []
        
        try:
            # í˜ì´ì§€ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            WebDriverWait(driver, 10).until(
                lambda d: d.find_elements(By.CSS_SELECTOR, 'h3.news_ttl, [class*="result"], div[class*="news"]')
            )
            
            # ì¶”ê°€ ëŒ€ê¸° (ë™ì  ì»¨í…ì¸  ë¡œë”©)
            time.sleep(2)
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            # ë‰´ìŠ¤ ì œëª© ì°¾ê¸° - ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
            news_titles = soup.select('h3.news_ttl')
            print(f"  ğŸ“Š h3.news_ttlë¡œ {len(news_titles)}ê°œ ë‰´ìŠ¤ ì œëª© ë°œê²¬")
            
            # ë‹¤ë¥¸ ì„ íƒìë¡œ ì¶”ê°€ ê²€ìƒ‰
            if len(news_titles) < max_items:
                for selector in ['div[class*="news"] h3', 'div[class*="news"] h2', 'a[href*="/news/"]']:
                    additional_titles = soup.select(selector)
                    for title in additional_titles:
                        if title not in news_titles and title.text.strip():
                            news_titles.append(title)
            
            # ë‰´ìŠ¤ ì •ë³´ ì¶”ì¶œ
            for idx, title_elem in enumerate(news_titles[:max_items * 2]):
                try:
                    title = title_elem.text.strip()
                    
                    # í•„í„°ë§
                    if not title or len(title) < 10:
                        continue
                    
                    exclude_keywords = ['AI ë‹µë³€', 'ì°¸ê³ ìë£Œ', 'ê´€ë ¨ ì§ˆë¬¸', 'Powered by', 'perplexity']
                    if any(keyword in title for keyword in exclude_keywords):
                        continue
                    
                    # URL ì¶”ì¶œ
                    news_url = self._extract_url(title_elem)
                    
                    news_list.append(NewsItem(
                        title=title,
                        date=date,
                        url=news_url,
                        source='ë§¤ì¼ê²½ì œ'
                    ))
                    
                    if self.debug:
                        print(f"  âœ… ë‰´ìŠ¤ {idx+1}: {title[:50]}...")
                        
                except Exception as e:
                    if self.debug:
                        print(f"  âš ï¸ ë‰´ìŠ¤ í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
                    
        except Exception as e:
            print(f"  âš ï¸ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            
        return news_list
    
    def _extract_url(self, title_elem) -> str:
        """ì œëª© ìš”ì†Œì—ì„œ URL ì¶”ì¶œ"""
        news_url = ''
        
        # ì œëª© ìš”ì†Œ ìì²´ê°€ ë§í¬ì¸ ê²½ìš°
        if title_elem.name == 'a' and title_elem.get('href'):
            news_url = title_elem['href']
        else:
            # ë¶€ëª¨ë‚˜ í˜•ì œ ìš”ì†Œì—ì„œ ë§í¬ ì°¾ê¸°
            parent = title_elem.parent
            while parent and parent.name != 'body':
                link = parent.find('a', href=True)
                if link and link.get('href'):
                    news_url = link['href']
                    break
                parent = parent.parent
        
        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if news_url and not news_url.startswith('http'):
            if news_url.startswith('//'):
                news_url = 'https:' + news_url
            else:
                news_url = 'https://www.mk.co.kr' + news_url
        
        return news_url
    
    def _load_more_news(self, driver, date: str, needed: int) -> List[NewsItem]:
        """ë” ë§ì€ ë‰´ìŠ¤ ë¡œë“œ"""
        additional_news = []
        
        print(f"  ğŸ“Œ ì¶”ê°€ ë‰´ìŠ¤ ë¡œë“œ ì‹œë„...")
        
        # 1. ìŠ¤í¬ë¡¤ì„ ë‚´ë ¤ì„œ ë” ë§ì€ ë‰´ìŠ¤ ë¡œë“œ
        for _ in range(3):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1.5)
        
        # 2. ë”ë³´ê¸° ë²„íŠ¼ ì°¾ê¸°
        try:
            more_button_clicked = False
            
            # JavaScriptë¡œ ë”ë³´ê¸° ë²„íŠ¼ ì°¾ê¸°
            buttons = driver.find_elements(By.TAG_NAME, 'button') + driver.find_elements(By.TAG_NAME, 'a')
            for button in buttons:
                if 'ë”ë³´ê¸°' in button.text and button.is_displayed():
                    button.click()
                    print(f"  âœ… ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
                    more_button_clicked = True
                    time.sleep(2)
                    break
            
            # 3. í˜ì´ì§€ ì¬íŒŒì‹±
            if more_button_clicked:
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                additional_news = self._parse_search_results(driver, date, needed)
                
        except Exception as e:
            if self.debug:
                print(f"  âš ï¸ ë”ë³´ê¸° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return additional_news
    
    def _remove_duplicates(self, news_items: List[NewsItem]) -> List[NewsItem]:
        """ì¤‘ë³µ ë‰´ìŠ¤ ì œê±°"""
        seen_titles = set()
        unique_items = []
        
        for item in news_items:
            title_key = item.title[:50]
            if title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_items.append(item)
        
        return unique_items
