"""
AI model training and prediction utilities
Enhanced with complete features from backtest_engine and train_ai_model
"""

import pandas as pd
import numpy as np
import json
import os
import warnings
from datetime import datetime, timedelta
from typing import Optional, Tuple
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score
from sklearn.utils.class_weight import compute_class_weight
from sklearn.utils import resample
import lightgbm as lgb

from ..data.fetcher import get_data_fetcher
from ..data.preprocessor import create_technical_features

warnings.filterwarnings('ignore')


class AIModelManager:
    """AI ëª¨ë¸ ê´€ë¦¬ í´ë˜ìŠ¤ - ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ ë° ë…ë¦½ í›ˆë ¨ ëª¨ë“ˆì˜ ëª¨ë“  ê¸°ëŠ¥ ì ìš©"""

    def __init__(self):
        self.data_fetcher = get_data_fetcher()
        self.model = None
        self.model_metadata = {}

    def _safe_import_smote(self):
        """SMOTEë¥¼ ì•ˆì „í•˜ê²Œ import"""
        try:
            from imblearn.over_sampling import SMOTE
            return SMOTE
        except ImportError:
            print("âš ï¸ imbalanced-learn ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   pip install imbalanced-learn ìœ¼ë¡œ ì„¤ì¹˜í•˜ê±°ë‚˜")
            print("   ê¸°ì¡´ ë¦¬ìƒ˜í”Œë§ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return None

    def _balance_classes_with_smote(self, X_train: np.ndarray, y_train: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """SMOTEë¥¼ ì‚¬ìš©í•œ í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²° (ì•ˆì „í•œ ì²˜ë¦¬)"""
        SMOTE = self._safe_import_smote()

        if SMOTE is not None:
            try:
                # SMOTE ì ìš© (k_neighborsë¥¼ ë°ì´í„° í¬ê¸°ì— ë§ê²Œ ì¡°ì •)
                X_train_class0 = X_train[y_train == 0]
                X_train_class1 = X_train[y_train == 1]
                min_class_size = min(len(X_train_class0), len(X_train_class1))
                k_neighbors = min(5, min_class_size - 1) if min_class_size > 1 else 1

                if k_neighbors >= 1:
                    smote = SMOTE(random_state=42, k_neighbors=k_neighbors)
                    X_train, y_train = smote.fit_resample(X_train, y_train)
                    print(f"ğŸ“Š SMOTE ì ìš© ì™„ë£Œ: ê· í˜• ë°ì´í„° ìƒì„±")
                    return X_train, y_train.astype(int)
                else:
                    print("âš ï¸ SMOTE ì ìš© ë¶ˆê°€ (ë°ì´í„° ë¶€ì¡±), ê¸°ì¡´ ë°©ë²• ì‚¬ìš©")
            except Exception as e:
                print(f"âš ï¸ SMOTE ì ìš© ì‹¤íŒ¨: {e}")

        # SMOTE ì—†ê±°ë‚˜ ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ë¦¬ìƒ˜í”Œë§ ë°©ë²• ì‚¬ìš©
        return self._balance_classes_traditional(X_train, y_train)

    def _balance_classes_traditional(self, X_train: np.ndarray, y_train: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ì „í†µì ì¸ ë°©ë²•ìœ¼ë¡œ í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°"""
        print("ğŸ“Š ê¸°ì¡´ ë¦¬ìƒ˜í”Œë§ ë°©ë²• ì‚¬ìš©")

        # ê° í´ë˜ìŠ¤ë³„ë¡œ ë°ì´í„° ë¶„ë¦¬
        X_train_class0 = X_train[y_train == 0]
        X_train_class1 = X_train[y_train == 1]

        # ê· í˜•ì¡íŒ ìƒ˜í”Œë§
        if len(X_train_class0) > len(X_train_class1):
            # í´ë˜ìŠ¤ 0ì´ ë” ë§ìœ¼ë©´ ì–¸ë”ìƒ˜í”Œë§
            X_train_class0_resampled = resample(X_train_class0, n_samples=int(len(X_train_class1) * 1.5),
                                                random_state=42)
            X_train_class1_resampled = X_train_class1
        else:
            # í´ë˜ìŠ¤ 1ì´ ë” ë§ìœ¼ë©´ ì–¸ë”ìƒ˜í”Œë§
            X_train_class0_resampled = X_train_class0
            X_train_class1_resampled = resample(X_train_class1, n_samples=int(len(X_train_class0) * 1.5),
                                                random_state=42)

        # ì¬ê²°í•©
        X_train = np.vstack([X_train_class0_resampled, X_train_class1_resampled])
        y_train = np.hstack([
            np.zeros(len(X_train_class0_resampled)),
            np.ones(len(X_train_class1_resampled))
        ])

        # ì…”í”Œ
        shuffle_idx = np.random.permutation(len(X_train))
        X_train = X_train[shuffle_idx]
        y_train = y_train[shuffle_idx].astype(int)

        return X_train, y_train

    def prepare_training_data_until(self, end_date, lookback_days=1000):
        """íŠ¹ì • ë‚ ì§œê¹Œì§€ì˜ AI ëª¨ë¸ í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„ (ê°•í™”ëœ ë²„ì „, Look-ahead bias ë°©ì§€)"""
        print(f"ğŸ“š {end_date}ê¹Œì§€ AI í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

        try:
            # ì¢…ë£Œ ë‚ ì§œ ì´ì „ì˜ ë°ì´í„°ë§Œ ìˆ˜ì§‘ (1000ì¼)
            end_date_pd = pd.to_datetime(end_date)
            start_date_pd = end_date_pd - timedelta(days=lookback_days)

            print(f"   ğŸ“… ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„: {start_date_pd.strftime('%Y-%m-%d')} ~ {end_date_pd.strftime('%Y-%m-%d')}")

            # ë‚ ì§œë³„ë¡œ ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ (ê³¼ê±° ë°ì´í„°ë§Œ)
            all_data = []
            current_date = start_date_pd
            collected_days = 0
            max_collect_days = min(lookback_days, 365)  # ìµœëŒ€ 1ë…„ì¹˜ë§Œ ìˆ˜ì§‘

            while current_date <= end_date_pd and collected_days < max_collect_days:
                if current_date.weekday() < 5:  # í‰ì¼ë§Œ
                    try:
                        # ë°ì´í„° fetcherë¥¼ í†µí•´ ìˆ˜ì§‘ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” pykrx ì‚¬ìš©)
                        daily_data = self.data_fetcher.get_market_data_by_date(current_date.strftime('%Y-%m-%d'))

                        if not daily_data.empty and daily_data['trade_amount'].sum() > 0:
                            daily_data['timestamp'] = current_date.strftime('%Y-%m-%d')
                            all_data.append(daily_data)
                            collected_days += 1

                    except Exception as e:
                        pass  # ë°ì´í„° ì—†ëŠ” ë‚ ì§œëŠ” ìŠ¤í‚µ

                current_date += timedelta(days=1)

            if not all_data:
                print("âŒ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                return None, None

            historical_data = pd.concat(all_data, ignore_index=True)
            historical_data['timestamp'] = pd.to_datetime(historical_data['timestamp'])

            print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(historical_data)}ê°œ ë ˆì½”ë“œ, {historical_data['ticker'].nunique()}ê°œ ì¢…ëª©")

            # ë‚ ì§œ ë²”ìœ„ í™•ì¸
            print(f"   ğŸ“… ë°ì´í„° ê¸°ê°„: {historical_data['timestamp'].min()} ~ {historical_data['timestamp'].max()}")

            # í•µì‹¬ í”¼ì²˜ë§Œ ì„ íƒ (25ê°œë¡œ í™•ëŒ€)
            feature_columns = [
                # í•µì‹¬ ìˆ˜ìµë¥ 
                'return_1d', 'return_3d', 'return_5d', 'return_10d', 'return_20d',

                # í•µì‹¬ ì´ë™í‰ê·  ë¹„ìœ¨
                'price_ma_ratio_5', 'price_ma_ratio_10', 'price_ma_ratio_20',

                # RSI
                'rsi_14',

                # ê±°ë˜ëŸ‰
                'volume_ratio_5d', 'volume_ratio_20d',

                # ë³€ë™ì„±
                'volatility_10d', 'volatility_20d',

                # ë³¼ë¦°ì € ë°´ë“œ
                'bb_position', 'bb_width',

                # MACD
                'macd_histogram',

                # ìŠ¤í† ìºìŠ¤í‹±
                'stoch_k',

                # ëª¨ë©˜í…€
                'price_momentum_5', 'price_momentum_10',

                # ì§€ì§€ì €í•­
                'low_proximity',

                # ì¶”ê°€ ì§€í‘œ
                'candle_streak', 'volume_price_corr', 'price_acceleration', 'round_number_proximity'
            ]

            all_features = []
            all_targets = []
            data_quality_stats = {
                'total_tickers': 0,
                'qualified_tickers': 0,
                'total_samples': 0,
                'skipped_insufficient_data': 0,
                'skipped_insufficient_features': 0,
                'skipped_insufficient_valid': 0
            }

            for ticker in historical_data['ticker'].unique():
                data_quality_stats['total_tickers'] += 1
                ticker_data = historical_data[historical_data['ticker'] == ticker].sort_values('timestamp')

                if len(ticker_data) < 30:  # ì¢…ëª©ë³„ ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­
                    data_quality_stats['skipped_insufficient_data'] += 1
                    continue

                # ê°•í™”ëœ ê¸°ìˆ ì  ì§€í‘œ ìƒì„±
                ticker_data = create_technical_features(ticker_data.copy())

                # ë‹¤ì¤‘ ê¸°ê°„ ë¯¸ë˜ ìˆ˜ìµë¥  ê³„ì‚°
                for future_days in [3, 5, 7]:
                    ticker_data[f'future_{future_days}d_return'] = ticker_data['close'].shift(-future_days) / \
                                                                   ticker_data['close'] - 1

                # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì‚¬ìš©
                valid_data = ticker_data.dropna()

                if len(valid_data) < 20:  # ì¢…ëª©ë³„ ìµœì†Œ ìœ íš¨ ìƒ˜í”Œ
                    data_quality_stats['skipped_insufficient_valid'] += 1
                    continue

                data_quality_stats['qualified_tickers'] += 1

                available_features = [col for col in feature_columns if col in valid_data.columns]

                if len(available_features) < 10:  # ìµœì†Œ í”¼ì²˜ ìˆ˜ (15ê°œ ì¤‘ 10ê°œ ì´ìƒ)
                    data_quality_stats['skipped_insufficient_features'] += 1
                    continue

                features = valid_data[available_features].values

                # ê°œì„ ëœ íƒ€ê²Ÿ ìƒì„±: ì•ˆì •ì„± ì¤‘ì‹¬ ì´ì§„ ë¶„ë¥˜
                future_5d_return = valid_data['future_5d_return']

                # ğŸ¯ íƒ€ê²Ÿ ì¬ì •ì˜ - ì•ˆì •ì„± ì¤‘ì‹¬ ì ‘ê·¼
                # ìˆ˜ìˆ˜ë£Œ 0.3% Ã— 2 = 0.6% + ìŠ¬ë¦¬í”¼ì§€ ê³ ë ¤í•˜ì—¬ 1.5% ì´ìƒì„ ì˜ë¯¸ìˆëŠ” ìˆ˜ìµìœ¼ë¡œ ì •ì˜
                # ê¸°ì¡´ 1% â†’ 1.5%ë¡œ ìƒí–¥ ì¡°ì • (ë” ì—„ê²©í•œ ê¸°ì¤€ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°)

                # 1ë‹¨ê³„: ê¸°ë³¸ ìˆ˜ìµë¥  ê¸°ì¤€ ìƒí–¥
                basic_profit_threshold = 0.015  # 1.5%

                # ğŸ¯ 2ë‹¨ê³„: ì•ˆì •ì„± ì¡°ê±´ ì¶”ê°€ (ì ì§„ì  ë„ì…)
                try:
                    # ë³€ë™ì„± ê³„ì‚° (5ì¼ê°„ ì¼ë³„ ìˆ˜ìµë¥ ì˜ í‘œì¤€í¸ì°¨)
                    volatility_5d = valid_data['return_1d'].rolling(5).std()
                    volatility_median = volatility_5d.median()

                    # ê¸‰ê²©í•œ í•˜ë½ ë°©ì§€ (5ì¼ê°„ ìµœëŒ€ í•˜ë½ë¥  ì²´í¬)
                    min_return_5d = valid_data['return_1d'].rolling(5).min()

                    # ì•ˆì •ì„± ê¸°ë°˜ ì¡°ê±´ë“¤
                    basic_profit = future_5d_return >= basic_profit_threshold  # 1.5% ì´ìƒ ìˆ˜ìµ
                    stable_volatility = volatility_5d <= volatility_median  # ì¤‘ê°„ ì´í•˜ ë³€ë™ì„±
                    no_major_crash = min_return_5d >= -0.05  # 5ì¼ê°„ ìµœëŒ€ 5% í•˜ë½ê¹Œì§€ë§Œ

                    # ìµœì¢… ì•ˆì •ì„± íƒ€ê²Ÿ: ëª¨ë“  ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê²½ìš°ë§Œ 1
                    stable_targets = basic_profit & stable_volatility & no_major_crash

                    # ì•ˆì •ì„± íƒ€ê²Ÿì˜ ìœ íš¨ì„± ê²€ì¦
                    if len(stable_targets.dropna()) > len(valid_data) * 0.7:  # 70% ì´ìƒ ìœ íš¨í•œ ê²½ìš°ë§Œ
                        targets = np.where(stable_targets.fillna(False), 1, 0)
                    else:
                        # ì•ˆì •ì„± ì¡°ê±´ì´ ë„ˆë¬´ ì—„ê²©í•˜ë©´ ê¸°ë³¸ íƒ€ê²Ÿ ì‚¬ìš©
                        targets = np.where(future_5d_return >= basic_profit_threshold, 1, 0)

                except Exception as e:
                    # ì•ˆì •ì„± ê³„ì‚° ì‹¤íŒ¨ì‹œ ê¸°ë³¸ íƒ€ê²Ÿìœ¼ë¡œ ëŒ€ì²´
                    targets = np.where(future_5d_return >= basic_profit_threshold, 1, 0)

                # ë¯¸ë˜ ë°ì´í„°ê°€ ì—†ëŠ” ë§ˆì§€ë§‰ 7ê°œ ì œì™¸
                if len(features) > 7:
                    features = features[:-7]
                    targets = targets[:-7]

                    all_features.extend(features)
                    all_targets.extend(targets)
                    data_quality_stats['total_samples'] += len(features)

            if len(all_features) < 200:  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ê°•í™”
                print("âŒ ì¶©ë¶„í•œ í•™ìŠµ ë°ì´í„° í™•ë³´ ì‹¤íŒ¨")
                print(f"ğŸ“Š ìˆ˜ì§‘ëœ ìƒ˜í”Œ: {len(all_features)}ê°œ (ìµœì†Œ 200ê°œ í•„ìš”)")
                return None, None

            print(f"âœ… í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
            print(f"   ğŸ“Š ì´ ìƒ˜í”Œ: {len(all_features)}ê°œ")
            print(f"   ğŸ“ˆ ë¶„ì„ ì¢…ëª©: {data_quality_stats['qualified_tickers']}/{data_quality_stats['total_tickers']}ê°œ")
            print(f"   ğŸ¯ íƒ€ê²Ÿ ë¶„í¬: {np.bincount(all_targets) if all_targets else []}")
            print(f"   ğŸ“ í”¼ì²˜ ìˆ˜: {len(feature_columns)}ê°œ")
            print(f"   âŒ ìŠ¤í‚µ ì›ì¸:")
            print(f"      - ë°ì´í„° ë¶€ì¡±: {data_quality_stats['skipped_insufficient_data']}ê°œ")
            print(f"      - ìœ íš¨ ìƒ˜í”Œ ë¶€ì¡±: {data_quality_stats['skipped_insufficient_valid']}ê°œ")
            print(f"      - í”¼ì²˜ ë¶€ì¡±: {data_quality_stats['skipped_insufficient_features']}ê°œ")

            return np.array(all_features), np.array(all_targets)

        except Exception as e:
            print(f"âŒ ë°ì´í„° ì¤€ë¹„ ì˜¤ë¥˜: {e}")
            return None, None

    def train_ai_model_at_date(self, end_date):
        """íŠ¹ì • ë‚ ì§œ ì‹œì ì—ì„œ AI ëª¨ë¸ í›ˆë ¨ (ê°•í™”ëœ ë²„ì „)"""
        print(f"ğŸ¤– {end_date} ì‹œì  AI ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")

        # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
        X, y = self.prepare_training_data_until(end_date)

        if X is None or len(X) < 200:
            print("âŒ í•™ìŠµ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ëª¨ë¸ í›ˆë ¨ ë¶ˆê°€")
            return None

        try:
            # ë°ì´í„° ë¶„í•  (ì‹œê³„ì—´ ê³ ë ¤)
            # í•™ìŠµ:ê²€ì¦:í…ŒìŠ¤íŠ¸ = 70:15:15
            n_samples = len(X)
            train_end = int(n_samples * 0.7)
            val_end = int(n_samples * 0.85)

            X_train = X[:train_end]
            y_train = y[:train_end]
            X_val = X[train_end:val_end]
            y_val = y[train_end:val_end]
            X_test = X[val_end:]
            y_test = y[val_end:]

            # í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ SMOTE ì˜¤ë²„ìƒ˜í”Œë§ (í›ˆë ¨ ë°ì´í„°ë§Œ)
            X_train, y_train = self._balance_classes_with_smote(X_train, y_train)

            # ì…”í”Œ (ê³µí†µ)
            shuffle_idx = np.random.permutation(len(X_train))
            X_train = X_train[shuffle_idx]
            y_train = y_train[shuffle_idx].astype(int)

            print(f"ğŸ“Š ë°ì´í„° ë¶„í• : í›ˆë ¨({len(X_train)}) / ê²€ì¦({len(X_val)}) / í…ŒìŠ¤íŠ¸({len(X_test)})")

            # ê°œì„ ëœ LightGBM íŒŒë¼ë¯¸í„° (ì´ì§„ ë¶„ë¥˜)
            lgb_params = {
                'objective': 'binary',
                'metric': 'binary_logloss',
                'num_leaves': 31,
                'learning_rate': 0.05,
                'feature_fraction': 0.8,
                'bagging_fraction': 0.8,
                'bagging_freq': 5,
                'min_data_in_leaf': 30,
                'lambda_l1': 0.1,
                'lambda_l2': 0.1,
                'min_gain_to_split': 0.05,
                'max_depth': 6,
                'verbose': -1,
                'random_state': 42,
                'force_col_wise': True,
                'is_unbalance': True,
                'boost_from_average': True,
            }

            # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ìˆ˜ë™ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ ì ìš©)
            class_weights = compute_class_weight(
                'balanced',
                classes=np.unique(y_train),
                y=y_train
            )

            print(f"ğŸ“Š í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {dict(zip(np.unique(y_train), class_weights))}")

            # ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ìƒì„±
            sample_weights = np.array([class_weights[label] for label in y_train])

            # ë°ì´í„°ì…‹ ìƒì„± (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)
            train_data = lgb.Dataset(X_train, label=y_train, weight=sample_weights)
            valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

            # ëª¨ë¸ í›ˆë ¨ (ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ ì™„í™”)
            model = lgb.train(
                lgb_params,
                train_data,
                valid_sets=[valid_data],
                num_boost_round=500,  # ì¦ê°€
                callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(50)]
            )

            # ë‹¤ì¤‘ ì„±ëŠ¥ í‰ê°€ (ì´ì§„ ë¶„ë¥˜)
            y_pred_val_proba = model.predict(X_val)
            y_pred_test_proba = model.predict(X_test)

            # ì´ì§„ ë¶„ë¥˜ ì˜ˆì¸¡ (ì„ê³„ê°’ ê°•í™”: 0.5 â†’ 0.7)
            y_pred_val = (y_pred_val_proba > 0.7).astype(int)
            y_pred_test = (y_pred_test_proba > 0.7).astype(int)

            val_accuracy = accuracy_score(y_val, y_pred_val)
            test_accuracy = accuracy_score(y_test, y_pred_test)

            print(f"âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
            print(f"ğŸ“Š ê²€ì¦ ì •í™•ë„: {val_accuracy:.3f}")
            print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.3f}")

            # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„
            print(f"ğŸ“Š ê²€ì¦ ë°ì´í„° í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬:")
            print(f"   ì‹¤ì œ: {np.bincount(y_val)}")
            print(f"   ì˜ˆì¸¡: {np.bincount(y_pred_val)}")

            # ì¶”ê°€ ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥
            try:
                auc_score = roc_auc_score(y_test, y_pred_test_proba)
                print(f"ğŸ“Š AUC ì ìˆ˜: {auc_score:.3f}")
            except:
                auc_score = 0.5

            # ëª¨ë¸ í’ˆì§ˆ ê²€ì¦ (ì´ì§„ ë¶„ë¥˜)
            f1 = f1_score(y_test, y_pred_test)
            precision = precision_score(y_test, y_pred_test)
            recall = recall_score(y_test, y_pred_test)

            model_quality_score = 0

            # 1. AUC ì ìˆ˜ (0-30ì )
            model_quality_score += min(auc_score * 30, 30) if 'auc_score' in locals() else 15

            # 2. F1 ì ìˆ˜ (0-30ì )
            model_quality_score += f1 * 30

            # 3. ì •ë°€ë„ (0-20ì ) - ê±°ì§“ ì–‘ì„±ì„ ì¤„ì´ëŠ” ê²ƒì´ ì¤‘ìš”
            model_quality_score += precision * 20

            # 4. ì¬í˜„ìœ¨ (0-20ì )
            model_quality_score += recall * 20

            print(f"ğŸ† ëª¨ë¸ í’ˆì§ˆ ì ìˆ˜: {model_quality_score:.1f}/100")
            print(f"   ğŸ“Š AUC: {auc_score:.3f}" if 'auc_score' in locals() else "   ğŸ“Š AUC: N/A")
            print(f"   ğŸ“Š ì •í™•ë„: {test_accuracy:.3f}")
            print(f"   ğŸ“Š ì •ë°€ë„: {precision:.3f}")
            print(f"   ğŸ“Š ì¬í˜„ìœ¨: {recall:.3f}")
            print(f"   ğŸ“Š F1 ì ìˆ˜: {f1:.3f}")

            # í˜¼ë™ í–‰ë ¬ ì¶œë ¥
            cm = confusion_matrix(y_test, y_pred_test)
            print(f"   ğŸ“Š í˜¼ë™ í–‰ë ¬:")
            print(f"      ì˜ˆì¸¡ 0    ì˜ˆì¸¡ 1")
            print(f"   ì‹¤ì œ 0: {cm[0, 0]:6d} {cm[0, 1]:6d}")
            print(f"   ì‹¤ì œ 1: {cm[1, 0]:6d} {cm[1, 1]:6d}")

            # ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì €ì¥
            model.model_quality_score = model_quality_score
            model.test_accuracy = test_accuracy
            model.train_date = end_date

            return model

        except Exception as e:
            print(f"âŒ ëª¨ë¸ í›ˆë ¨ ì˜¤ë¥˜: {e}")
            import traceback
            print(traceback.format_exc())
            return None

    def get_ai_prediction_score(self, ticker: str, current_date=None, model=None) -> float:
        """AI ëª¨ë¸ì„ ì‚¬ìš©í•œ ì˜ˆì¸¡ ì ìˆ˜ (ë‹¤ì¤‘ í´ë˜ìŠ¤ ëŒ€ì‘, backtest_engineê³¼ ë™ì¼)"""
        try:
            if model is None:
                model = self.model or self.load_ai_model()
                if model is None:
                    return 0.3

            # í˜„ì¬ ë‚ ì§œê¹Œì§€ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©
            data = self.data_fetcher.get_past_data_enhanced(ticker, n=50)  # ì¶©ë¶„í•œ ë°ì´í„° í™•ë³´
            if data.empty or len(data) < 30:
                return 0.3  # ë°ì´í„° ë¶€ì¡±ì‹œ ë‚®ì€ ì ìˆ˜

            # í˜„ì¬ ë‚ ì§œ ì´í›„ ë°ì´í„° ì œê±° (ë°±í…ŒìŠ¤íŠ¸ ì‹œ)
            if current_date:
                current_date_pd = pd.to_datetime(current_date)
                data = data[pd.to_datetime(data['timestamp']) <= current_date_pd].copy()
                if len(data) < 30:
                    return 0.3

            # ê°•í™”ëœ ê¸°ìˆ ì  ì§€í‘œ ìƒì„±
            data = create_technical_features(data)
            latest = data.iloc[-1]

            # í•µì‹¬ í”¼ì²˜ ì¶”ì¶œ (í›ˆë ¨ì‹œì™€ ë™ì¼í•œ ìˆœì„œ - 25ê°œ)
            feature_columns = [
                'return_1d', 'return_3d', 'return_5d', 'return_10d', 'return_20d',
                'price_ma_ratio_5', 'price_ma_ratio_10', 'price_ma_ratio_20',
                'rsi_14', 'volume_ratio_5d', 'volume_ratio_20d',
                'volatility_10d', 'volatility_20d',
                'bb_position', 'bb_width', 'macd_histogram', 'stoch_k',
                'price_momentum_5', 'price_momentum_10', 'low_proximity',
                'candle_streak', 'volume_price_corr', 'price_acceleration', 'round_number_proximity'
            ]

            # í”¼ì²˜ ë²¡í„° ìƒì„± (ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ê°•í™”)
            features = []
            missing_features = 0

            for col in feature_columns:
                if col in latest.index and not pd.isna(latest[col]) and np.isfinite(latest[col]):
                    features.append(float(latest[col]))
                else:
                    # í”¼ì²˜ë³„ ê¸°ë³¸ê°’ ì„¤ì •
                    if 'return' in col:
                        features.append(0.0)
                    elif 'ratio' in col:
                        features.append(1.0)
                    elif 'rsi' in col:
                        features.append(50.0)
                    elif 'bb_position' in col:
                        features.append(0.0)
                    elif 'volume_ratio' in col:
                        features.append(1.0)
                    elif 'volatility' in col:
                        features.append(0.02)
                    elif 'candle_streak' in col:
                        features.append(0.0)
                    elif 'corr' in col:
                        features.append(0.0)
                    elif 'acceleration' in col:
                        features.append(0.0)
                    elif 'proximity' in col:
                        features.append(0.5)
                    else:
                        features.append(0.0)
                    missing_features += 1

            # ë„ˆë¬´ ë§ì€ í”¼ì²˜ê°€ ëˆ„ë½ë˜ë©´ ë‚®ì€ ì ìˆ˜ ë°˜í™˜
            if missing_features > len(feature_columns) * 0.3:  # 30% ì´ìƒ ëˆ„ë½
                print(f"âš ï¸ {ticker}: í”¼ì²˜ ëˆ„ë½ ë¹„ìœ¨ ë†’ìŒ ({missing_features}/{len(feature_columns)})")
                return 0.2

            # AI ì˜ˆì¸¡ (ì´ì§„ ë¶„ë¥˜)
            prediction_prob = model.predict([features])[0]  # ìˆ˜ìµ í™•ë¥  (0~1)

            # ì˜ˆì¸¡ í™•ë¥ ì„ ê·¸ëŒ€ë¡œ ì ìˆ˜ë¡œ ì‚¬ìš©
            final_score = float(prediction_prob)

            # ì‹ ë¢°ë„ ë³´ì •
            # ê·¹ë‹¨ì ì¸ ì˜ˆì¸¡(0.8 ì´ìƒ ë˜ëŠ” 0.2 ì´í•˜)ì— ë³´ë„ˆìŠ¤
            if prediction_prob > 0.8 or prediction_prob < 0.2:
                confidence = abs(prediction_prob - 0.5) * 2  # 0~1 ë²”ìœ„
                final_score = final_score * 0.8 + confidence * 0.2

            return float(final_score)

        except Exception as e:
            print(f"âŒ AI ì˜ˆì¸¡ ì˜¤ë¥˜ ({ticker}): {e}")
            return 0.2

    def prepare_training_data(self, lookback_days: int = 1000) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
        """AI ëª¨ë¸ í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„ (ë…ë¦½ í›ˆë ¨ ëª¨ë“ˆê³¼ ë™ì¼)"""
        print("ğŸ“š AI ëª¨ë¸ í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

        try:
            # ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘
            historical_data = self.data_fetcher.get_past_data_total(n=lookback_days)

            if len(historical_data) < 300:
                print("âŒ í•™ìŠµ ë°ì´í„° ë¶€ì¡± (ìµœì†Œ 300ì¼ í•„ìš”)")
                return None, None

            # í•µì‹¬ í”¼ì²˜ë§Œ ì„ íƒ (25ê°œ)
            feature_columns = [
                'return_1d', 'return_3d', 'return_5d', 'return_10d', 'return_20d',
                'price_ma_ratio_5', 'price_ma_ratio_10', 'price_ma_ratio_20',
                'rsi_14', 'volume_ratio_5d', 'volume_ratio_20d',
                'volatility_10d', 'volatility_20d',
                'bb_position', 'bb_width', 'macd_histogram', 'stoch_k',
                'price_momentum_5', 'price_momentum_10', 'low_proximity',
                'candle_streak', 'volume_price_corr', 'price_acceleration', 'round_number_proximity'
            ]

            all_features = []
            all_targets = []
            data_quality_stats = {
                'total_tickers': 0,
                'qualified_tickers': 0,
                'total_samples': 0,
                'skipped_insufficient_data': 0,
                'skipped_insufficient_features': 0,
                'skipped_insufficient_valid': 0
            }

            # ì¢…ëª©ë³„ë¡œ ë°ì´í„° ì²˜ë¦¬
            for ticker in historical_data['ticker'].unique():
                data_quality_stats['total_tickers'] += 1
                ticker_data = historical_data[historical_data['ticker'] == ticker].sort_values('timestamp')

                if len(ticker_data) < 30:
                    data_quality_stats['skipped_insufficient_data'] += 1
                    continue

                # ê°•í™”ëœ ê¸°ìˆ ì  ì§€í‘œ ìƒì„±
                ticker_data = create_technical_features(ticker_data.copy())

                # ë‹¤ì¤‘ ê¸°ê°„ ë¯¸ë˜ ìˆ˜ìµë¥  ê³„ì‚°
                for future_days in [3, 5, 7]:
                    ticker_data[f'future_{future_days}d_return'] = ticker_data['close'].shift(-future_days) / \
                                                                   ticker_data['close'] - 1

                # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì‚¬ìš©
                valid_data = ticker_data.dropna()

                if len(valid_data) < 20:
                    data_quality_stats['skipped_insufficient_valid'] += 1
                    continue

                data_quality_stats['qualified_tickers'] += 1

                available_features = [col for col in feature_columns if col in valid_data.columns]

                if len(available_features) < 15:
                    data_quality_stats['skipped_insufficient_features'] += 1
                    continue

                features = valid_data[available_features].values

                # ğŸ¯ ê°œì„ ëœ íƒ€ê²Ÿ ìƒì„±: ì•ˆì •ì„± ì¤‘ì‹¬ ì´ì§„ ë¶„ë¥˜ (ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ê³¼ ë™ì¼)
                future_5d_return = valid_data['future_5d_return']

                # ğŸ¯ íƒ€ê²Ÿ ì¬ì •ì˜ - ì•ˆì •ì„± ì¤‘ì‹¬ ì ‘ê·¼
                # ìˆ˜ìˆ˜ë£Œ 0.3% Ã— 2 = 0.6% + ìŠ¬ë¦¬í”¼ì§€ ê³ ë ¤í•˜ì—¬ 1.5% ì´ìƒì„ ì˜ë¯¸ìˆëŠ” ìˆ˜ìµìœ¼ë¡œ ì •ì˜
                # ê¸°ì¡´ 1% â†’ 1.5%ë¡œ ìƒí–¥ ì¡°ì • (ë” ì—„ê²©í•œ ê¸°ì¤€ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°)

                # 1ë‹¨ê³„: ê¸°ë³¸ ìˆ˜ìµë¥  ê¸°ì¤€ ìƒí–¥
                basic_profit_threshold = 0.015  # 1.5%

                # ğŸ¯ 2ë‹¨ê³„: ì•ˆì •ì„± ì¡°ê±´ ì¶”ê°€ (ì ì§„ì  ë„ì…)
                try:
                    # ë³€ë™ì„± ê³„ì‚° (5ì¼ê°„ ì¼ë³„ ìˆ˜ìµë¥ ì˜ í‘œì¤€í¸ì°¨)
                    volatility_5d = valid_data['return_1d'].rolling(5).std()
                    volatility_median = volatility_5d.median()

                    # ê¸‰ê²©í•œ í•˜ë½ ë°©ì§€ (5ì¼ê°„ ìµœëŒ€ í•˜ë½ë¥  ì²´í¬)
                    min_return_5d = valid_data['return_1d'].rolling(5).min()

                    # ì•ˆì •ì„± ê¸°ë°˜ ì¡°ê±´ë“¤
                    basic_profit = future_5d_return >= basic_profit_threshold  # 1.5% ì´ìƒ ìˆ˜ìµ
                    stable_volatility = volatility_5d <= volatility_median  # ì¤‘ê°„ ì´í•˜ ë³€ë™ì„±
                    no_major_crash = min_return_5d >= -0.05  # 5ì¼ê°„ ìµœëŒ€ 5% í•˜ë½ê¹Œì§€ë§Œ

                    # ìµœì¢… ì•ˆì •ì„± íƒ€ê²Ÿ: ëª¨ë“  ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê²½ìš°ë§Œ 1
                    stable_targets = basic_profit & stable_volatility & no_major_crash

                    # ì•ˆì •ì„± íƒ€ê²Ÿì˜ ìœ íš¨ì„± ê²€ì¦
                    if len(stable_targets.dropna()) > len(valid_data) * 0.7:  # 70% ì´ìƒ ìœ íš¨í•œ ê²½ìš°ë§Œ
                        targets = np.where(stable_targets.fillna(False), 1, 0)
                    else:
                        # ì•ˆì •ì„± ì¡°ê±´ì´ ë„ˆë¬´ ì—„ê²©í•˜ë©´ ê¸°ë³¸ íƒ€ê²Ÿ ì‚¬ìš©
                        targets = np.where(future_5d_return >= basic_profit_threshold, 1, 0)

                except Exception as e:
                    # ì•ˆì •ì„± ê³„ì‚° ì‹¤íŒ¨ì‹œ ê¸°ë³¸ íƒ€ê²Ÿìœ¼ë¡œ ëŒ€ì²´
                    targets = np.where(future_5d_return >= basic_profit_threshold, 1, 0)

                # ë¯¸ë˜ ë°ì´í„°ê°€ ì—†ëŠ” ë§ˆì§€ë§‰ 7ê°œ ì œì™¸
                if len(features) > 7:
                    features = features[:-7]
                    targets = targets[:-7]

                    all_features.extend(features)
                    all_targets.extend(targets)
                    data_quality_stats['total_samples'] += len(features)

            if len(all_features) < 200:
                print("âŒ ì¶©ë¶„í•œ í•™ìŠµ ë°ì´í„° í™•ë³´ ì‹¤íŒ¨")
                print(f"ğŸ“Š ìˆ˜ì§‘ëœ ìƒ˜í”Œ: {len(all_features)}ê°œ (ìµœì†Œ 200ê°œ í•„ìš”)")
                return None, None

            print(f"âœ… í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
            print(f"   ğŸ“Š ì´ ìƒ˜í”Œ: {len(all_features)}ê°œ")
            print(f"   ğŸ“ˆ ë¶„ì„ ì¢…ëª©: {data_quality_stats['qualified_tickers']}/{data_quality_stats['total_tickers']}ê°œ")
            print(f"   ğŸ¯ íƒ€ê²Ÿ ë¶„í¬: {np.bincount(all_targets)}")
            print(f"   ğŸ“ í”¼ì²˜ ìˆ˜: {len(feature_columns)}ê°œ")
            print(f"   âŒ ìŠ¤í‚µ ì›ì¸:")
            print(f"      - ë°ì´í„° ë¶€ì¡±: {data_quality_stats['skipped_insufficient_data']}ê°œ")
            print(f"      - ìœ íš¨ ìƒ˜í”Œ ë¶€ì¡±: {data_quality_stats['skipped_insufficient_valid']}ê°œ")
            print(f"      - í”¼ì²˜ ë¶€ì¡±: {data_quality_stats['skipped_insufficient_features']}ê°œ")

            return np.array(all_features), np.array(all_targets)

        except Exception as e:
            print(f"âŒ ë°ì´í„° ì¤€ë¹„ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None, None

    def train_ai_model(self) -> Optional[lgb.Booster]:
        """AI ëª¨ë¸ í›ˆë ¨ (ë…ë¦½ í›ˆë ¨ ëª¨ë“ˆê³¼ ë™ì¼)"""
        print("ğŸ¤– AI ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")

        # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
        X, y = self.prepare_training_data()

        if X is None or len(X) < 200:
            print("âŒ í•™ìŠµ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ëª¨ë¸ í›ˆë ¨ ë¶ˆê°€")
            return None

        try:
            # ë°ì´í„° ë¶„í•  (ì‹œê³„ì—´ ê³ ë ¤)
            n_samples = len(X)
            train_end = int(n_samples * 0.7)
            val_end = int(n_samples * 0.85)

            X_train = X[:train_end]
            y_train = y[:train_end]
            X_val = X[train_end:val_end]
            y_val = y[train_end:val_end]
            X_test = X[val_end:]
            y_test = y[val_end:]

            # í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²° (SMOTE ì˜¤ë²„ìƒ˜í”Œë§ ìš°ì„  ì ìš©)
            try:
                from imblearn.over_sampling import SMOTE

                # SMOTE ì ìš© (k_neighborsë¥¼ ë°ì´í„° í¬ê¸°ì— ë§ê²Œ ì¡°ì •)
                X_train_class0 = X_train[y_train == 0]
                X_train_class1 = X_train[y_train == 1]
                min_class_size = min(len(X_train_class0), len(X_train_class1))
                k_neighbors = min(5, min_class_size - 1) if min_class_size > 1 else 1

                if k_neighbors >= 1:
                    smote = SMOTE(random_state=42, k_neighbors=k_neighbors)
                    X_train, y_train = smote.fit_resample(X_train, y_train)
                    print(f"ğŸ“Š SMOTE ì ìš© ì™„ë£Œ: ê· í˜• ë°ì´í„° ìƒì„± ({len(X_train)} ìƒ˜í”Œ)")
                else:
                    raise ValueError("SMOTE ì ìš© ë¶ˆê°€")

            except Exception as e:
                print(f"âš ï¸ SMOTE ì ìš© ì‹¤íŒ¨, ê¸°ì¡´ ë¦¬ìƒ˜í”Œë§ ì‚¬ìš©: {e}")
                # ê¸°ì¡´ ë¦¬ìƒ˜í”Œë§ ë°©ì‹
                X_train_class0 = X_train[y_train == 0]
                X_train_class1 = X_train[y_train == 1]

                if len(X_train_class0) > len(X_train_class1):
                    X_train_class0_resampled = resample(X_train_class0, n_samples=int(len(X_train_class1) * 1.5),
                                                        random_state=42)
                    X_train_class1_resampled = X_train_class1
                else:
                    X_train_class0_resampled = X_train_class0
                    X_train_class1_resampled = resample(X_train_class1, n_samples=int(len(X_train_class0) * 1.5),
                                                        random_state=42)

                X_train = np.vstack([X_train_class0_resampled, X_train_class1_resampled])
                y_train = np.hstack([
                    np.zeros(len(X_train_class0_resampled)),
                    np.ones(len(X_train_class1_resampled))
                ])

            # ì…”í”Œ (ê³µí†µ)
            shuffle_idx = np.random.permutation(len(X_train))
            X_train = X_train[shuffle_idx]
            y_train = y_train[shuffle_idx].astype(int)

            print(f"ğŸ“Š ë°ì´í„° ë¶„í• : í›ˆë ¨({len(X_train)}) / ê²€ì¦({len(X_val)}) / í…ŒìŠ¤íŠ¸({len(X_test)})")

            # LightGBM íŒŒë¼ë¯¸í„°
            lgb_params = {
                'objective': 'binary',
                'metric': 'binary_logloss',
                'num_leaves': 31,
                'learning_rate': 0.05,
                'feature_fraction': 0.8,
                'bagging_fraction': 0.8,
                'bagging_freq': 5,
                'min_data_in_leaf': 30,
                'lambda_l1': 0.1,
                'lambda_l2': 0.1,
                'min_gain_to_split': 0.05,
                'max_depth': 6,
                'verbose': -1,
                'random_state': 42,
                'force_col_wise': True,
                'is_unbalance': True,
                'boost_from_average': True,
            }

            # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
            class_weights = compute_class_weight(
                'balanced',
                classes=np.unique(y_train),
                y=y_train
            )

            print(f"ğŸ“Š í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {dict(zip(np.unique(y_train), class_weights))}")
            sample_weights = np.array([class_weights[label] for label in y_train])

            # ë°ì´í„°ì…‹ ìƒì„±
            train_data = lgb.Dataset(X_train, label=y_train, weight=sample_weights)
            valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

            # ëª¨ë¸ í›ˆë ¨
            model = lgb.train(
                lgb_params,
                train_data,
                valid_sets=[valid_data],
                num_boost_round=500,
                callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(50)]
            )

            # ì„±ëŠ¥ í‰ê°€
            y_pred_val_proba = model.predict(X_val)
            y_pred_test_proba = model.predict(X_test)

            # ì´ì§„ ë¶„ë¥˜ ì˜ˆì¸¡ (ì„ê³„ê°’ ê°•í™”: 0.5 â†’ 0.7)
            y_pred_val = (y_pred_val_proba > 0.7).astype(int)
            y_pred_test = (y_pred_test_proba > 0.7).astype(int)

            val_accuracy = accuracy_score(y_val, y_pred_val)
            test_accuracy = accuracy_score(y_test, y_pred_test)

            print(f"âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
            print(f"ğŸ“Š ê²€ì¦ ì •í™•ë„: {val_accuracy:.3f}")
            print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.3f}")

            print(f"ğŸ“Š ê²€ì¦ ë°ì´í„° í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬:")
            print(f"   ì‹¤ì œ: {np.bincount(y_val)}")
            print(f"   ì˜ˆì¸¡: {np.bincount(y_pred_val)}")

            # ì¶”ê°€ ì„±ëŠ¥ ì§€í‘œ
            try:
                auc_score = roc_auc_score(y_test, y_pred_test_proba)
                print(f"ğŸ“Š AUC ì ìˆ˜: {auc_score:.3f}")
            except:
                auc_score = 0.5

            f1 = f1_score(y_test, y_pred_test)
            precision = precision_score(y_test, y_pred_test)
            recall = recall_score(y_test, y_pred_test)

            # ëª¨ë¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            model_quality_score = 0
            model_quality_score += min(auc_score * 30, 30)
            model_quality_score += f1 * 30
            model_quality_score += precision * 20
            model_quality_score += recall * 20

            print(f"ğŸ† ëª¨ë¸ í’ˆì§ˆ ì ìˆ˜: {model_quality_score:.1f}/100")
            print(f"   ğŸ“Š AUC: {auc_score:.3f}")
            print(f"   ğŸ“Š ì •í™•ë„: {test_accuracy:.3f}")
            print(f"   ğŸ“Š ì •ë°€ë„: {precision:.3f}")
            print(f"   ğŸ“Š ì¬í˜„ìœ¨: {recall:.3f}")
            print(f"   ğŸ“Š F1 ì ìˆ˜: {f1:.3f}")

            # í˜¼ë™ í–‰ë ¬
            cm = confusion_matrix(y_test, y_pred_test)
            print(f"   ğŸ“Š í˜¼ë™ í–‰ë ¬:")
            print(f"      ì˜ˆì¸¡ 0    ì˜ˆì¸¡ 1")
            print(f"   ì‹¤ì œ 0: {cm[0, 0]:6d} {cm[0, 1]:6d}")
            print(f"   ì‹¤ì œ 1: {cm[1, 0]:6d} {cm[1, 1]:6d}")

            # ëª¨ë¸ ì €ì¥
            model.save_model('ai_price_prediction_model.txt')
            print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: ai_price_prediction_model.txt")

            # ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì €ì¥
            model_metadata = {
                'train_date': datetime.now().isoformat(),
                'model_type': 'LightGBM_Binary_Independent',
                'train_samples': len(X_train),
                'test_accuracy': float(test_accuracy),
                'val_accuracy': float(val_accuracy),
                'model_quality_score': float(model_quality_score),
                'feature_count': X_train.shape[1],
                'class_count': 2,
                'auc_score': float(auc_score),
                'f1_score': float(f1),
                'precision': float(precision),
                'recall': float(recall),
                'class_distribution': {
                    'train': np.bincount(y_train).tolist(),
                    'test': np.bincount(y_test).tolist()
                },
                'prediction_distribution': {
                    'val': np.bincount(y_pred_val).tolist(),
                    'test': np.bincount(y_pred_test).tolist()
                }
            }

            with open('ai_model_metadata.json', 'w') as f:
                json.dump(model_metadata, f, indent=2)

            self.model = model
            return model

        except Exception as e:
            print(f"âŒ ëª¨ë¸ í›ˆë ¨ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None

    def load_ai_model(self) -> Optional[lgb.Booster]:
        """ì €ì¥ëœ AI ëª¨ë¸ ë¡œë“œ (ê°•í™”ëœ ë²„ì „)"""
        try:
            if os.path.exists('ai_price_prediction_model.txt'):
                model = lgb.Booster(model_file='ai_price_prediction_model.txt')

                # ëª¨ë¸ ë©”íƒ€ë°ì´í„° ê²€ì¦
                self._load_model_metadata()

                self.model = model
                return model
            else:
                print("ğŸ“ ì €ì¥ëœ ëª¨ë¸ì´ ì—†ì–´ ìƒˆë¡œ í›ˆë ¨í•©ë‹ˆë‹¤...")
                return self.train_ai_model()

        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            print("ğŸ”„ ìƒˆ ëª¨ë¸ í›ˆë ¨ ì‹œë„...")
            return self.train_ai_model()

    def _load_model_metadata(self) -> None:
        """ëª¨ë¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        try:
            with open('ai_model_metadata.json', 'r') as f:
                metadata = json.load(f)

            print(f"ğŸ“… ëª¨ë¸ í›ˆë ¨ì¼: {metadata.get('train_date', 'Unknown')}")
            print(f"ğŸ“Š ëª¨ë¸ í’ˆì§ˆ: {metadata.get('model_quality_score', 0):.1f}/100")
            print(f"ğŸ”¢ í´ë˜ìŠ¤ ìˆ˜: {metadata.get('class_count', 'Unknown')}")
            print(f"ğŸ“ í”¼ì²˜ ìˆ˜: {metadata.get('feature_count', 'Unknown')}")

            # ëª¨ë¸ì´ ë„ˆë¬´ ì˜¤ë˜ë˜ì—ˆëŠ”ì§€ í™•ì¸ (7ì¼ ì´ìƒ)
            train_date = metadata.get('train_date')
            if train_date:
                train_datetime = datetime.fromisoformat(train_date.replace('Z', '+00:00'))
                days_old = (datetime.now() - train_datetime).days

                if days_old > 7:
                    print(f"âš ï¸ ëª¨ë¸ì´ {days_old}ì¼ ì „ì— í›ˆë ¨ë¨. ì¬í›ˆë ¨ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")

                    # ìë™ ì¬í›ˆë ¨ ì—¬ë¶€ ê²°ì •
                    if days_old > 14:  # 2ì£¼ ì´ìƒ ëœ ëª¨ë¸ì€ ìë™ ì¬í›ˆë ¨
                        print("ğŸ”„ ëª¨ë¸ì´ ë„ˆë¬´ ì˜¤ë˜ë˜ì–´ ìë™ ì¬í›ˆë ¨ ì‹œì‘...")
                        self.train_ai_model()
                        return

            self.model_metadata = metadata
            return

        except FileNotFoundError:
            print("âš ï¸ ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì—†ìŒ")
            return
        except Exception as e:
            print(f"âš ï¸ ë©”íƒ€ë°ì´í„° ì½ê¸° ì˜¤ë¥˜: {e}")
            return


# ì „ì—­ AI ëª¨ë¸ ë§¤ë‹ˆì € (ì‹±ê¸€í†¤ íŒ¨í„´)
_ai_manager_instance = None


def get_ai_manager() -> AIModelManager:
    """AI ëª¨ë¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _ai_manager_instance
    if _ai_manager_instance is None:
        _ai_manager_instance = AIModelManager()
    return _ai_manager_instance


# í¸ì˜ í•¨ìˆ˜ë“¤
def train_ai_model() -> Optional[lgb.Booster]:
    """AI ëª¨ë¸ í›ˆë ¨"""
    manager = get_ai_manager()
    return manager.train_ai_model()


def load_ai_model() -> Optional[lgb.Booster]:
    """AI ëª¨ë¸ ë¡œë“œ"""
    manager = get_ai_manager()
    return manager.load_ai_model()


def get_ai_prediction_score(ticker: str) -> float:
    """AI ì˜ˆì¸¡ ì ìˆ˜ ê³„ì‚°"""
    manager = get_ai_manager()
    return manager.get_ai_prediction_score(ticker)


def prepare_training_data(lookback_days: int = 1000) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
    """í•™ìŠµ ë°ì´í„° ì¤€ë¹„"""
    manager = get_ai_manager()
    return manager.prepare_training_data(lookback_days)
